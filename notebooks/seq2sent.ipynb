{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  INFORMATION:\n",
    "  \n",
    "    Course     : EEE485/585\n",
    "    Name       : Can Kocagil\n",
    "    ID         : 21602218\n",
    "    E-mail     : can.kocagil@ug.bilkent.edu.tr\n",
    "    Assignment : Final Report\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import (\n",
    "    print_function,\n",
    "    division\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import abc\n",
    "import os \n",
    "import re\n",
    "import sys \n",
    "sys.path.append('./src')\n",
    "\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y, \n",
    "    check_array\n",
    ")\n",
    "\n",
    "\n",
    "from typing import (\n",
    "    Callable,\n",
    "    Iterable,\n",
    "    List,\n",
    "    Union,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "from preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    StandardScaler,\n",
    "    #OneHotEncoder\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    Pipeline,\n",
    "    Classifier,\n",
    "    json_print,\n",
    "    timeit,\n",
    "    random_seed,\n",
    "    #save_obj,\n",
    "    #load_obj,\n",
    ")\n",
    "\n",
    "from classifier import (\n",
    "    KNeighborsClassifier,\n",
    "    MultiNominalNaiveBayes,\n",
    "    BernaulliNaiveBayes,\n",
    "    LogisticRegression\n",
    ")\n",
    "\n",
    "from metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "#from neural_network import (\n",
    "#    RNN,\n",
    "#    LSTM,\n",
    "#    GRU\n",
    "#)\n",
    "#from activations import Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X:pd.DataFrame, split_size:list = [0.7, 0.1, 0.2], random_state:int = 42):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X.copy())\n",
    "\n",
    "    split_size = np.array(split_size)\n",
    "\n",
    "    assert np.isclose(split_size.sum(), 1.0), f\"Split ratios should sum to 1\"\n",
    "\n",
    "    cum_splits = split_size.cumsum()\n",
    "    len_X = len(X)\n",
    "\n",
    "    indices = np.ceil(cum_splits[:2] * len_X)\n",
    "\n",
    "    train, val, test = np.split(\n",
    "        X.sample(frac = 1, random_state = random_state),\n",
    "        indices.astype(np.int)\n",
    "    )\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def save_obj(obj: object, path: str = None) -> None:\n",
    "    \"\"\" Saves Python Object as pickle\"\"\"\n",
    "    with open(path + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(path: str = None) -> object:\n",
    "    \"\"\" Loads Python Object from pickle\"\"\"\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_embedding(path: str = 'glove.6B.50d.txt', save_path: str = '../word_embeddings/love.6B.50d') -> dict:\n",
    "    \"\"\" Given the path of the embedding file, return a dictionary of embedding\"\"\"\n",
    "    \n",
    "    assert os.path.exists(save_path), f\"The path {path} is not exists\"\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(path, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(\n",
    "            values[1:], \n",
    "            dtype='float32'\n",
    "        )\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    if save_path:\n",
    "        save_obj(\n",
    "            embeddings_index,\n",
    "            save_path\n",
    "        )\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "def max_token_size(text):\n",
    "    return np.max(\n",
    "    text.apply(\n",
    "        lambda doc: len(doc.split())\n",
    "    )\n",
    ")\n",
    "\n",
    "class PretrainedWord2Vec:\n",
    "    def __init__(self, embeddings_index, max_length):\n",
    "        self.max_length = max_length\n",
    "        self.embeddings_index = embeddings_index\n",
    "\n",
    "    def doc_to_vec(self, doc):\n",
    "        return np.array([\n",
    "            self.embeddings_index[token] if token in self.embeddings_index.keys() else self.embeddings_index['unknown'] for token in doc.split()\n",
    "        ])\n",
    "\n",
    "    def fit(self, df:pd.DataFrame) -> None:\n",
    "        if not isinstance(df, pd.Series):\n",
    "            df = pd.Series(df.copy())\n",
    "\n",
    "        self.vectors = df.apply(\n",
    "            self.zero_padding\n",
    "        ).apply(\n",
    "            self.doc_to_vec\n",
    "        )\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return np.array(\n",
    "            self.vectors.tolist(), \n",
    "            dtype=np.float\n",
    "        )\n",
    "\n",
    "    def zero_padding(self, doc):\n",
    "        doc = doc.split()\n",
    "        if len(doc) < self.max_length:\n",
    "            length_doc = len(doc)\n",
    "            num_expand = self.max_length - length_doc\n",
    "            doc.extend(['0'] * num_expand)\n",
    "        return \" \".join(doc)\n",
    "\n",
    "\n",
    "def accuracy(preds: Iterable[list or np.ndarray], labels: Iterable[list or np.ndarray], scale:bool = True) -> np.float:\n",
    "    \"\"\"Given the labels and predictions, calculate accuracy score. \"\"\"\n",
    "    return np.mean(preds == labels) * 100 if scale else np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \"\"\" Necessary activation functions for recurrent neural network \"\"\"\n",
    "    \n",
    "    def relu_alternative(self,X):\n",
    "        \"\"\" Rectified linear unit activation(ReLU). \"\"\"\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def ReLU(self,X):\n",
    "        \"\"\" Rectified linear unit activation(ReLU). Most time efficient version.\n",
    "        \"\"\"\n",
    "        return (abs(X) + X) / 2\n",
    "\n",
    "    def relu_another(self,X):\n",
    "        \"\"\" Rectified linear unit activation(ReLU). \"\"\"\n",
    "        return X * (X > 0)\n",
    "\n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def tanh_manuel(self,X):\n",
    "        \"\"\" Hyperbolic tangent activation(tanh). \"\"\"      \n",
    "        return (np.exp(X) - np.exp(-X))/(np.exp(X) + np.exp(-X))\n",
    "\n",
    "    def sigmoid(self,X):\n",
    "        \"\"\"Sigmoidal activation.\"\"\"\n",
    "        c = np.clip(X, -700, 700)\n",
    "        return 1 / (1 + np.exp(-c))\n",
    "\n",
    "    def softmax(self, X, axis=-1):\n",
    "        \"\"\" Stable version of softmax classifier, note that column sum is equal to 1. \"\"\"\n",
    "        e_x = np.exp(X - np.max(X, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "        \n",
    "    def softmax_stable(self,X):\n",
    "        \"\"\" Less stable version of softmax activation \"\"\"\n",
    "        e_x = np.exp(X - np.max(X))\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def ReLUDerivative(self,X): \n",
    "        \"\"\" The derivative of the ReLU function w.r.t. given input. \"\"\"\n",
    "        return 1 * (X > 0)\n",
    "\n",
    "    def ReLU_grad(self,X):\n",
    "        \"\"\" The derivative of the ReLU function w.r.t. given input. \"\"\"\n",
    "        X[X<=0] = 0\n",
    "        X[X>1] = 1\n",
    "        return X\n",
    "\n",
    "    def dReLU(self,X):  \n",
    "        \"\"\" The derivative of the ReLU function w.r.t. given input. \"\"\"     \n",
    "        return np.where(X <= 0, 0, 1)\n",
    "\n",
    "    def dtanh(self,X): \n",
    "        \"\"\" The derivative of the tanh function w.r.t. given input. \"\"\"       \n",
    "        return  1-(np.tanh(X)**2)\n",
    "\n",
    "    def dsigmoid(self,X):\n",
    "        \"\"\" The derivative of the sigmoid function w.r.t. given input. \"\"\"\n",
    "        return self.sigmoid(X) * (1-self.sigmoid(X))    \n",
    "    \n",
    "    def softmax_stable_gradient(self,soft_out):           \n",
    "        return soft_out * (1 - soft_out)\n",
    "\n",
    "    def softmax_grad(self,softmax):        \n",
    "        s = softmax.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    def softmax_gradient(self,Sz):\n",
    "        \"\"\"Computes the gradient of the softmax function.\n",
    "        z: (T, 1) array of input values where the gradient is computed. T is the\n",
    "        number of output classes.\n",
    "        Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\n",
    "        is DjSi - the partial derivative of Si w.r.t. input j.\n",
    "        \"\"\"\n",
    "        \n",
    "        # -SjSi can be computed using an outer product between Sz and itself. Then\n",
    "        # we add back Si for the i=j cases by adding a diagonal matrix with the\n",
    "        # values of Si on its diagonal.\n",
    "        D = -np.outer(Sz, Sz) + np.diag(Sz.flatten())\n",
    "        return D\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X):\n",
    "        self.n_values = np.max(X) + 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.eye(self.n_values)[X]\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot(X, num_classes):\n",
    "        return np.squeeze(np.eye(num_classes)[X.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\" Recurrent Neural Network (RNN). RNN encapsulates all necessary logic for training the network. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim = 3,\n",
    "        hidden_dim = 128, \n",
    "        seq_len = 150, \n",
    "        learning_rate = 1e-1, \n",
    "        mom_coeff = 0.85, \n",
    "        batch_size = 32, \n",
    "        output_class = 6\n",
    "    ):\n",
    "\n",
    "        \"\"\" Initialization of weights/biases and other configurable parameters.  \"\"\"\n",
    "        np.random.seed(150)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unfold case T = 150 :\n",
    "        self.seq_len = seq_len\n",
    "        self.output_class = output_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.mom_coeff = mom_coeff\n",
    "\n",
    "        # Xavier uniform scaler :\n",
    "        Xavier = lambda fan_in,fan_out : math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "        lim_inp2hid = Xavier(self.input_dim,self.hidden_dim)\n",
    "        self.W1 = np.random.uniform(-lim_inp2hid,lim_inp2hid,(self.input_dim,self.hidden_dim))\n",
    "        self.B1 = np.random.uniform(-lim_inp2hid,lim_inp2hid,(1,self.hidden_dim))\n",
    "\n",
    "        lim_hid2hid = Xavier(self.hidden_dim,self.hidden_dim)\n",
    "        self.W1_rec= np.random.uniform(-lim_hid2hid,lim_hid2hid,(self.hidden_dim,self.hidden_dim))\n",
    "\n",
    "        lim_hid2out = Xavier(self.hidden_dim,self.output_class)\n",
    "        self.W2 = np.random.uniform(-lim_hid2out,lim_hid2out,(self.hidden_dim,self.output_class))\n",
    "        self.B2 = np.random.uniform(-lim_inp2hid,lim_inp2hid,(1,self.output_class))\n",
    "\n",
    "        # To keep track loss and accuracy score :     \n",
    "        self.train_loss, self.test_loss, self.train_acc, self.test_acc = [],[],[],[]\n",
    "        \n",
    "        # Storing previous momentum updates :\n",
    "        self.prev_updates = {\n",
    "            'W1'       : 0,\n",
    "            'B1'       : 0,\n",
    "            'W1_rec'   : 0,\n",
    "            'W2'       : 0,\n",
    "            'B2'       : 0\n",
    "        }\n",
    "\n",
    "\n",
    "    def forward(self, X) -> tuple:\n",
    "        \"\"\" \n",
    "        Forward propagation of the RNN through time.\n",
    "        \n",
    "        * X_state is the input across all time steps\n",
    "        * hidden_state is the hidden stages across time\n",
    "        * probs is the probabilities of each outputs, i.e. outputs of softmax.\n",
    "        \n",
    "        Returns:\n",
    "            * (X_state, hidden_state, probs) as a tuple.       \n",
    "        \"\"\" \n",
    "        X_state = dict()\n",
    "        hidden_state = dict()\n",
    "        output_state = dict()\n",
    "        probs = dict()\n",
    "\n",
    "        self.h_prev_state = np.zeros((1,self.hidden_dim))\n",
    "        hidden_state[-1] = np.copy(self.h_prev_state)\n",
    "\n",
    "        # Loop over time T:\n",
    "        for t in range(self.seq_len):\n",
    "\n",
    "            # Selecting first record with inputs, dimension = (batch_size, input_size)\n",
    "            X_state[t] = X[:, t]\n",
    "\n",
    "            # Recurrent hidden layer :\n",
    "            hidden_state[t] = np.tanh(\n",
    "                np.dot(X_state[t], self.W1) + np.dot(hidden_state[t-1], self.W1_rec) + self.B1\n",
    "            )\n",
    "\n",
    "            output_state[t] = np.dot(\n",
    "                hidden_state[t], self.W2\n",
    "            ) + self.B2\n",
    "\n",
    "            # Per class probabilites :\n",
    "            probs[t] = activations.softmax(output_state[t], axis=-1)\n",
    "\n",
    "        return X_state, hidden_state, probs\n",
    "        \n",
    "\n",
    "    def BPTT(self, cache, Y):\n",
    "        \"\"\"\n",
    "        Back propagation through time algorihm.\n",
    "        \n",
    "        Inputs:\n",
    "         * Cache = (X_state, hidden_state, probs)\n",
    "         * Y = desired output\n",
    "\n",
    "        Returns:\n",
    "            * Gradients w.r.t. all configurable elements\n",
    "        \"\"\"\n",
    "\n",
    "        X_state, hidden_state, probs = cache\n",
    "\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dW1, dW1_rec, dW2 = np.zeros_like(self.W1), np.zeros_like(self.W1_rec), np.zeros_like(self.W2)\n",
    "\n",
    "        dB1, dB2 = np.zeros_like(self.B1), np.zeros_like(self.B2)\n",
    "\n",
    "        dhnext = np.zeros_like(hidden_state[0])\n",
    "\n",
    "        dy = np.copy(probs[self.seq_len - 1])      \n",
    "        dy[np.arange(len(Y)), np.argmax(Y, axis = 1)] -= 1\n",
    "        \n",
    "        dB2 += np.sum(dy, axis = 0, keepdims = True)\n",
    "        dW2 += np.dot(hidden_state[self.seq_len - 1].T, dy)\n",
    "\n",
    "        for t in reversed(range(1, self.seq_len)):\n",
    "\n",
    "\n",
    "            dh = np.dot(dy, self.W2.T) + dhnext\n",
    "        \n",
    "            dhrec = (1 - (hidden_state[t] * hidden_state[t])) * dh\n",
    "\n",
    "            dB1 += np.sum(dhrec, axis = 0, keepdims = True)\n",
    "            \n",
    "            dW1 += np.dot(X_state[t].T, dhrec)\n",
    "            \n",
    "            dW1_rec += np.dot(hidden_state[t-1].T, dhrec)\n",
    "\n",
    "            dhnext = np.dot(dhrec, self.W1_rec.T)\n",
    "\n",
    "\n",
    "        for grad in [dW1,dB1,dW1_rec,dW2,dB2]:\n",
    "            np.clip(grad, -10, 10, out = grad)\n",
    "\n",
    "\n",
    "        return [dW1, dB1, dW1_rec, dW2, dB2]    \n",
    "        \n",
    "    def earlyStopping(self, ce_train, ce_val, ce_threshold, acc_train, acc_val, acc_threshold):\n",
    "        return any([\n",
    "            ce_train - ce_val < ce_threshold, \n",
    "            acc_train - acc_val > acc_threshold\n",
    "        ])\n",
    "    \n",
    "    def CategoricalCrossEntropy(self, labels, preds):\n",
    "        \"\"\" Computes cross entropy between labels and model's predictions \"\"\"\n",
    "        predictions = np.clip(preds, 1e-12, 1. - 1e-12)\n",
    "        N = predictions.shape[0]         \n",
    "        return -np.sum(labels * np.log(predictions + 1e-9)) / N\n",
    "\n",
    "    def step(self, grads, momentum = True):\n",
    "        \"\"\" SGD w/o Momentum on mini batches \"\"\"\n",
    "\n",
    "        if momentum:\n",
    "            \n",
    "            delta_W1 = -self.learning_rate * grads[0] +  self.mom_coeff * self.prev_updates['W1']\n",
    "            delta_B1 = -self.learning_rate * grads[1] +  self.mom_coeff * self.prev_updates['B1']  \n",
    "            delta_W1_rec = -self.learning_rate * grads[2] +  self.mom_coeff * self.prev_updates['W1_rec']\n",
    "            delta_W2 = -self.learning_rate * grads[3] +  self.mom_coeff * self.prev_updates['W2']              \n",
    "            delta_B2 = -self.learning_rate * grads[4] +  self.mom_coeff * self.prev_updates['B2']\n",
    "            \n",
    "            self.W1 += delta_W1\n",
    "            self.W1_rec += delta_W1_rec\n",
    "            self.W2 += delta_W2\n",
    "            self.B1 += delta_B1\n",
    "            self.B2 += delta_B2     \n",
    "\n",
    "            self.prev_updates['W1'] = delta_W1\n",
    "            self.prev_updates['W1_rec'] = delta_W1_rec\n",
    "            self.prev_updates['W2'] = delta_W2\n",
    "            self.prev_updates['B1'] = delta_B1\n",
    "            self.prev_updates['B2'] = delta_B2\n",
    "\n",
    "            self.learning_rate *= 0.9999\n",
    "\n",
    "    def fit(self, X, Y, X_val, y_val, epochs = 50, verbose = True, earlystopping = False):\n",
    "        \"\"\"\n",
    "        Given the traning dataset,their labels and number of epochs\n",
    "        fitting the model, and measure the performance\n",
    "        by validating training dataset.\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            print(f'Epoch : {epoch + 1}')\n",
    "\n",
    "            perm = np.random.permutation(X.shape[0])           \n",
    "            \n",
    "            for i in range(round(X.shape[0]/ self.batch_size)): \n",
    "\n",
    "                batch_start  =  i * self.batch_size\n",
    "                batch_finish = (i+1) * self.batch_size\n",
    "                index = perm[batch_start:batch_finish]\n",
    "                \n",
    "                X_feed = X[index]    \n",
    "                y_feed = Y[index]\n",
    "                \n",
    "                cache_train = self.forward(X_feed)                                                          \n",
    "                grads = self.BPTT(cache_train, y_feed)                \n",
    "                self.step(grads)\n",
    "\n",
    "            cross_loss_train = self.CategoricalCrossEntropy(y_feed, cache_train[2][self.seq_len - 1])\n",
    "            predictions_train = self.predict(X)\n",
    "            acc_train = accuracy(\n",
    "                np.argmax(Y, axis = 1),\n",
    "                predictions_train\n",
    "            )\n",
    "\n",
    "            _, __, probs_test = self.forward(X_val)\n",
    "            cross_loss_val = self.CategoricalCrossEntropy(y_val,probs_test[self.seq_len - 1])\n",
    "            predictions_val = np.argmax(probs_test[self.seq_len - 1], 1)\n",
    "            acc_val = accuracy(\n",
    "                np.argmax(y_val, axis = 1),\n",
    "                predictions_val\n",
    "            )\n",
    "            \n",
    "\n",
    "            if earlystopping:                \n",
    "                if self.earlyStopping(\n",
    "                    ce_train = cross_loss_train, \n",
    "                    ce_val = cross_loss_val, \n",
    "                    ce_threshold = 3.0,\n",
    "                    acc_train = acc_train,\n",
    "                    acc_val = acc_val,\n",
    "                    acc_threshold = 15\n",
    "                ): \n",
    "                    break\n",
    "\n",
    "            if verbose:\n",
    "\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Accuracy : {acc_train}\")\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Loss     : {cross_loss_train}\")\n",
    "                print('______________________________________________________________________________________\\n')                         \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Accuracy : {acc_val}\")                                        \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Loss     : {cross_loss_val}\")\n",
    "                print('______________________________________________________________________________________\\n')\n",
    "                \n",
    "            self.train_loss.append(cross_loss_train)              \n",
    "            self.test_loss.append(cross_loss_val) \n",
    "            self.train_acc.append(acc_train)              \n",
    "            self.test_acc.append(acc_val)\n",
    "\n",
    "    def predict(self,X):\n",
    "        _, __, probs = self.forward(X)\n",
    "        return np.argmax(\n",
    "            probs[self.seq_len - 1],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def history(self):\n",
    "        return {\n",
    "            'TrainLoss' : self.train_loss,\n",
    "            'TrainAcc'  : self.train_acc,\n",
    "            'TestLoss'  : self.test_loss,\n",
    "            'TestAcc'   : self.test_acc\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\" Long-Short Term Memory Recurrent neural network, encapsulates all necessary logic for training,\n",
    "        then built the hyperparameters and architecture of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "      self,\n",
    "      input_dim = 3,\n",
    "      hidden_dim = 100,\n",
    "      output_class = 6,\n",
    "      seq_len = 150,\n",
    "      batch_size = 30,\n",
    "      learning_rate = 1e-1,\n",
    "      mom_coeff = 0.85,\n",
    "      random_state = 150\n",
    "    ):\n",
    "        \"\"\" Initialization of weights/biases and other configurable parameters. \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unfold case T:\n",
    "        self.seq_len = seq_len\n",
    "        self.output_class = output_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.mom_coeff = mom_coeff\n",
    "\n",
    "        self.input_stack_dim = self.input_dim + self.hidden_dim\n",
    "        \n",
    "        # Xavier uniform scaler :\n",
    "        Xavier = lambda fan_in,fan_out : math.sqrt(6 / (fan_in + fan_out))\n",
    "\n",
    "        lim1 = Xavier(self.input_dim,self.hidden_dim)\n",
    "        self.W_f = np.random.uniform(-lim1,lim1,(self.input_stack_dim, self.hidden_dim))\n",
    "        self.B_f = np.random.uniform(-lim1,lim1,(1, self.hidden_dim))\n",
    "\n",
    "        self.W_i = np.random.uniform(-lim1,lim1,(self.input_stack_dim, self.hidden_dim))\n",
    "        self.B_i = np.random.uniform(-lim1,lim1,(1, self.hidden_dim))\n",
    "\n",
    "        self.W_c = np.random.uniform(-lim1,lim1,(self.input_stack_dim, self.hidden_dim))\n",
    "        self.B_c = np.random.uniform(-lim1,lim1,(1, self.hidden_dim))\n",
    "\n",
    "        self.W_o = np.random.uniform(-lim1,lim1,(self.input_stack_dim, self.hidden_dim))\n",
    "        self.B_o = np.random.uniform(-lim1,lim1,(1, self.hidden_dim))\n",
    "        \n",
    "        lim2 = Xavier(self.hidden_dim,self.output_class)\n",
    "        self.W = np.random.uniform(-lim2,lim2,(self.hidden_dim, self.output_class))\n",
    "        self.B = np.random.uniform(-lim2,lim2,(1, self.output_class))\n",
    "\n",
    "        # To keep track loss and accuracy score :     \n",
    "        self.train_loss, self.test_loss, self.train_acc, self.test_acc = [],[],[],[]\n",
    "        \n",
    "        # To keep previous updates in momentum :\n",
    "        self.previous_updates = [0] * 10\n",
    "        \n",
    "        # For AdaGrad:\n",
    "        self.cache = [0] * 10     \n",
    "        self.cache_rmsprop = [0] * 10\n",
    "        self.m = [0] * 10\n",
    "        self.v = [0] * 10\n",
    "        self.t = 1\n",
    "\n",
    "    def cell_forward(self,X, h_prev, C_prev):\n",
    "        \"\"\"\n",
    "        Takes input, previous hidden state and previous cell state, compute:\n",
    "          * Forget gate + Input gate + New candidate input + New cell state + \n",
    "          * output gate + hidden state. Then, classify by softmax.\n",
    "        \"\"\"\n",
    "\n",
    "        # Stacking previous hidden state vector with inputs:\n",
    "        stack = np.column_stack([X, h_prev])\n",
    "\n",
    "        # Forget gate:\n",
    "        forget_gate = activations.sigmoid(\n",
    "          np.dot(stack, self.W_f) + self.B_f\n",
    "        )\n",
    "  \n",
    "        # İnput gate:\n",
    "        input_gate = activations.sigmoid(\n",
    "          np.dot(stack, self.W_i) + self.B_i\n",
    "        )\n",
    "\n",
    "        # New candidate:\n",
    "        cell_bar = np.tanh(\n",
    "          np.dot(stack, self.W_c) + self.B_c\n",
    "        )\n",
    "\n",
    "        # New Cell state:\n",
    "        cell_state = forget_gate * C_prev + input_gate * cell_bar\n",
    "\n",
    "        # Output fate:\n",
    "        output_gate = activations.sigmoid(\n",
    "          np.dot(stack, self.W_o) + self.B_o\n",
    "        )\n",
    "\n",
    "        # Hidden state:\n",
    "        hidden_state = output_gate * np.tanh(cell_state)\n",
    "\n",
    "        # Classifiers (Softmax) :\n",
    "        dense = np.dot(hidden_state, self.W) + self.B\n",
    "        probs = activations.softmax(dense, axis=-1)\n",
    "\n",
    "        return (stack, forget_gate, input_gate, cell_bar, cell_state, output_gate, hidden_state, dense, probs)\n",
    "\n",
    "    def forward(self, X, h_prev, C_prev):\n",
    "        x_s, z_s, f_s, i_s = {}, {}, {}, {}\n",
    "        C_bar_s, C_s, o_s, h_s = {}, {}, {},{}\n",
    "        v_s, y_s = {}, {}\n",
    "\n",
    "\n",
    "        h_s[-1] = np.copy(h_prev)\n",
    "        C_s[-1] = np.copy(C_prev)\n",
    "\n",
    "        for t in range(self.seq_len):\n",
    "            x_s[t] = X[:,t,:]\n",
    "\n",
    "            z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t],v_s[t], y_s[t] = self.cell_forward(\n",
    "              x_s[t],\n",
    "              h_s[t-1],\n",
    "              C_s[t-1]\n",
    "            )\n",
    "\n",
    "        return (z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s, v_s, y_s)\n",
    "    \n",
    "    def BPTT(self, outs, Y):\n",
    "\n",
    "        z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s,v_s, y_s = outs\n",
    "\n",
    "        dW_f, dW_i, dW_c, dW_o, dW = np.zeros_like(self.W_f), np.zeros_like(self.W_i), np.zeros_like(self.W_c), np.zeros_like(self.W_o), np.zeros_like(self.W)\n",
    "\n",
    "        dB_f, dB_i, dB_c, dB_o, dB = np.zeros_like(self.B_f), np.zeros_like(self.B_i), np.zeros_like(self.B_c), np.zeros_like(self.B_o), np.zeros_like(self.B)\n",
    "\n",
    "        dh_next = np.zeros_like(h_s[0]) \n",
    "        dC_next = np.zeros_like(C_s[0])   \n",
    "\n",
    "        # w.r.t. softmax input\n",
    "        ddense = np.copy(y_s[self.seq_len - 1])\n",
    "        ddense[np.arange(len(Y)), np.argmax(Y, axis=1)] -= 1\n",
    "        #ddense[np.argmax(Y,1)] -=1\n",
    "        #ddense = y_s[149] - Y\n",
    "        # Softmax classifier's :\n",
    "        dW = np.dot(h_s[self.seq_len - 1].T,ddense)\n",
    "        dB = np.sum(ddense, axis = 0, keepdims = True)\n",
    "\n",
    "        # Backprop through time:\n",
    "        for t in reversed(range(1, self.seq_len)):           \n",
    "            \n",
    "            # Just equating more meaningful names\n",
    "            stack, forget_gate, input_gate, cell_bar, cell_state, output_gate, hidden_state, dense, probs = z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t],v_s[t], y_s[t]\n",
    "            C_prev = C_s[t-1]\n",
    "            \n",
    "            # w.r.t. softmax input\n",
    "            #ddense = np.copy(probs)\n",
    "            #ddense[np.arange(len(Y)),np.argmax(Y,1)] -= 1\n",
    "            #ddense[np.arange(len(Y)),np.argmax(Y,1)] -=1\n",
    "            # Softmax classifier's :\n",
    "            #dW += np.dot(hidden_state.T,ddense)\n",
    "            #dB += np.sum(ddense,axis = 0, keepdims = True)\n",
    "\n",
    "            # Output gate :\n",
    "            dh = np.dot(ddense, self.W.T) + dh_next            \n",
    "            do = dh * np.tanh(cell_state)\n",
    "            do = do * dsigmoid(output_gate)\n",
    "            dW_o += np.dot(stack.T,do)\n",
    "            dB_o += np.sum(do, axis = 0, keepdims = True)\n",
    "\n",
    "            # Cell state:\n",
    "            dC = np.copy(dC_next)\n",
    "            dC += dh * output_gate * activations.dtanh(cell_state)\n",
    "            dC_bar = dC * input_gate\n",
    "            dC_bar = dC_bar * dtanh(cell_bar) \n",
    "            dW_c += np.dot(stack.T, dC_bar)\n",
    "            dB_c += np.sum(dC_bar,axis = 0, keepdims = True)\n",
    "            \n",
    "            # Input gate:\n",
    "            di = dC * cell_bar\n",
    "            di = dsigmoid(input_gate) * di\n",
    "            dW_i += np.dot(stack.T, di)\n",
    "            dB_i += np.sum(di, axis = 0, keepdims = True)\n",
    "\n",
    "            # Forget gate:\n",
    "            df = dC * C_prev\n",
    "            df = df * dsigmoid(forget_gate) \n",
    "            dW_f += np.dot(stack.T, df)\n",
    "            dB_f += np.sum(df, axis = 0, keepdims = True)\n",
    "\n",
    "            dz = np.dot(df, self.W_f.T) + np.dot(di, self.W_i.T) + np.dot(dC_bar, self.W_c.T) + np.dot(do, self.W_o.T)\n",
    "\n",
    "            dh_next = dz[:, -self.hidden_dim:]\n",
    "            dC_next = forget_gate * dC\n",
    "        \n",
    "        # List of gradients :\n",
    "        grads = [dW, dB, dW_o, dB_o, dW_c, dB_c, dW_i, dB_i, dW_f, dB_f]\n",
    "\n",
    "        # Clipping gradients anyway\n",
    "        for grad in grads:\n",
    "            np.clip(grad, -15, 15, out = grad)\n",
    "\n",
    "        return h_s[self.seq_len - 1], C_s[self.seq_len -1 ], grads\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, X, Y, X_val, y_val, epochs = 50, optimizer = 'SGD', verbose = True, crossVal = False):\n",
    "        \"\"\"\n",
    "        Given the traning dataset,their labels and number of epochs\n",
    "        fitting the model, and measure the performance\n",
    "        by validating training dataset.\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            print(f'Epoch : {epoch + 1}')\n",
    "\n",
    "            perm = np.random.permutation(X.shape[0])           \n",
    "            h_prev, C_prev = np.zeros((self.batch_size, self.hidden_dim)), np.zeros((self.batch_size, self.hidden_dim))\n",
    "            \n",
    "            for i in range(round(X.shape[0] / self.batch_size) - 1): \n",
    "          \n",
    "                batch_start  =  i * self.batch_size\n",
    "                batch_finish = (i+1) * self.batch_size                \n",
    "                index = perm[batch_start:batch_finish]\n",
    "                \n",
    "                # Feeding random indexes:\n",
    "                X_feed = X[index]    \n",
    "                y_feed = Y[index]\n",
    "          \n",
    "                # Forward + BPTT + SGD:\n",
    "                cache_train = self.forward(X_feed, h_prev, C_prev)\n",
    "                h,c,grads = self.BPTT(cache_train, y_feed)\n",
    "\n",
    "                if optimizer == 'SGD':                                                                        \n",
    "                  self.SGD(grads)\n",
    "\n",
    "                elif optimizer == 'AdaGrad' :\n",
    "                  self.AdaGrad(grads)\n",
    "\n",
    "                elif optimizer == 'RMSprop':\n",
    "                  self.RMSprop(grads)\n",
    "                \n",
    "                elif optimizer == 'VanillaAdam':\n",
    "                  self.VanillaAdam(grads)\n",
    "                  \n",
    "                else:\n",
    "                  self.Adam(grads)\n",
    "\n",
    "                # Hidden state -------> Previous hidden state\n",
    "                # Cell state ---------> Previous cell state\n",
    "                h_prev, C_prev = h, c\n",
    "\n",
    "            # Training metrics calculations:\n",
    "            cross_loss_train = self.CategoricalCrossEntropy(y_feed, cache_train[8][self.seq_len - 1])\n",
    "            predictions_train = self.predict(X)\n",
    "            acc_train = accuracy(\n",
    "              np.argmax(Y, axis=1),\n",
    "              predictions_train\n",
    "            )\n",
    "\n",
    "            # Validation metrics calculations:\n",
    "            test_prevs = np.zeros((X_val.shape[0], self.hidden_dim))\n",
    "            _,__,___,____,_____,______,_______,________, probs_test = self.forward(X_val, test_prevs, test_prevs)\n",
    "            cross_loss_val = self.CategoricalCrossEntropy(y_val, probs_test[self.seq_len - 1])\n",
    "            \n",
    "            predictions_val = np.argmax(\n",
    "              probs_test[self.seq_len - 1],\n",
    "              axis = 1\n",
    "            )\n",
    "\n",
    "            acc_val = accuracy(\n",
    "              np.argmax(y_val,axis=1), \n",
    "              predictions_val\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Accuracy : {acc_train}\")\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Loss     : {cross_loss_train}\")\n",
    "                print('______________________________________________________________________________________\\n')                         \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Accuracy : {acc_val}\")                                        \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Loss     : {cross_loss_val}\")\n",
    "                print('______________________________________________________________________________________\\n')\n",
    "                \n",
    "            self.train_loss.append(cross_loss_train)              \n",
    "            self.test_loss.append(cross_loss_val) \n",
    "            self.train_acc.append(acc_train)              \n",
    "            self.test_acc.append(acc_val)\n",
    "      \n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Return all weights/biases in sequential order starting from end in list form.\n",
    "\n",
    "        \"\"\"        \n",
    "        return [self.W, self.B, self.W_o, self.B_o, self.W_c, self.B_c, self.W_i, self.B_i, self.W_f, self.B_f]\n",
    "\n",
    "\n",
    "    def SGD(self,grads):\n",
    "      \"\"\" Stochastic gradient descent with momentum on mini-batches. \"\"\"\n",
    "      prevs = []\n",
    "      for param,grad,prev_update in zip(self.params(),grads,self.previous_updates):            \n",
    "          delta = self.learning_rate * grad - self.mom_coeff * prev_update\n",
    "          param -= delta \n",
    "          prevs.append(delta)\n",
    "\n",
    "      self.previous_updates = prevs       \n",
    "\n",
    "      self.learning_rate *= 0.99999   \n",
    "\n",
    "    \n",
    "    def AdaGrad(self, grads):\n",
    "      \"\"\" AdaGrad adaptive optimization algorithm. \"\"\"         \n",
    "\n",
    "      i = 0\n",
    "      for param,grad in zip(self.params(), grads):\n",
    "\n",
    "        self.cache[i] += grad **2\n",
    "        param += -self.learning_rate * grad / (np.sqrt(self.cache[i]) + 1e-6)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    def RMSprop(self, grads, decay_rate = 0.9):\n",
    "      \"\"\" RMSprop adaptive optimization algorithm \"\"\"\n",
    "\n",
    "\n",
    "      for i, (param, grad) in enumerate(zip(self.params(), grads)):\n",
    "        self.cache_rmsprop[i] = decay_rate * self.cache_rmsprop[i] + (1-decay_rate) * grad **2\n",
    "        param += - self.learning_rate * grad / (np.sqrt(self.cache_rmsprop[i])+ 1e-6)\n",
    "        \n",
    "\n",
    "\n",
    "    def VanillaAdam(self, grads, beta1 = 0.9, beta2 = 0.999):\n",
    "        \"\"\" Adam optimizer, but bias correction is not implemented \"\"\"\n",
    "      \n",
    "\n",
    "        for i, (param, grad)  in enumerate(zip(self.params(), grads)):\n",
    "          self.m[i] = beta1 * self.m[i] + (1-beta1) * grad          \n",
    "          self.v[i] = beta2 * self.v[i] + (1-beta2) * grad **2  \n",
    "          param += -self.learning_rate * self.m[i] / (np.sqrt(self.v[i]) + 1e-8)\n",
    "\n",
    "\n",
    "    def Adam(self, grads, beta1 = 0.9, beta2 = 0.999):\n",
    "        \"\"\" Adam optimizer, bias correction is implemented.\n",
    "        \"\"\"\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(self.params(), grads)):\n",
    "          self.m[i] = beta1 * self.m[i] + (1-beta1) * grad          \n",
    "          self.v[i] = beta2 * self.v[i] + (1-beta2) * grad **2\n",
    "          m_corrected = self.m[i] / (1-beta1**self.t)\n",
    "          v_corrected = self.v[i] / (1-beta2**self.t)\n",
    "          param += -self.learning_rate * m_corrected / (np.sqrt(v_corrected) + 1e-8)\n",
    "\n",
    "        self.t +=1\n",
    "    \n",
    "    \n",
    "    def CategoricalCrossEntropy(self,labels,preds):\n",
    "        \"\"\" Computes cross entropy between labels and model's predictions \"\"\"\n",
    "        predictions = np.clip(preds, 1e-12, 1. - 1e-12)\n",
    "        N = predictions.shape[0]         \n",
    "        return -np.sum(labels * np.log(predictions + 1e-9)) / N\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\" Return predictions, (not one hot encoded format) \"\"\"\n",
    "\n",
    "        # Give zeros to hidden/cell states:\n",
    "        pasts = np.zeros((X.shape[0], self.hidden_dim))\n",
    "        _, __ ,___ ,____, _____, ______, _______, _______, probs = self.forward(X, pasts, pasts)\n",
    "        return np.argmax(probs[self.seq_len - 1], axis=1)\n",
    "\n",
    "    def history(self):\n",
    "        return {\n",
    "          'TrainLoss' : self.train_loss,\n",
    "          'TrainAcc'  : self.train_acc,\n",
    "          'TestLoss'  : self.test_loss,\n",
    "          'TestAcc'   : self.test_acc\n",
    "        }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"\n",
    "    Gater recurrent unit, encapsulates all necessary logic for training, \n",
    "    then built the hyperparameters and architecture of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "      self,\n",
    "      input_dim = 3,\n",
    "      hidden_dim = 128,\n",
    "      output_class = 6,\n",
    "      seq_len = 150,\n",
    "      batch_size = 32,\n",
    "      learning_rate = 1e-1,\n",
    "      mom_coeff = 0.85,\n",
    "      random_state = 32\n",
    "    ):\n",
    "        \"\"\" Initialization of weights/biases and other configurable parameters. \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unfold case T = 150 :\n",
    "        self.seq_len = seq_len\n",
    "        self.output_class = output_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.mom_coeff = mom_coeff\n",
    "\n",
    "        # Xavier uniform scaler :\n",
    "        Xavier = lambda fan_in,fan_out : math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "        lim1 = Xavier(self.input_dim,self.hidden_dim)\n",
    "        lim1_hid = Xavier(self.hidden_dim,self.hidden_dim)\n",
    "        self.W_z = np.random.uniform(-lim1,lim1,(self.input_dim,self.hidden_dim))\n",
    "        self.U_z = np.random.uniform(-lim1_hid,lim1_hid,(self.hidden_dim,self.hidden_dim))\n",
    "        self.B_z = np.random.uniform(-lim1,lim1,(1,self.hidden_dim))\n",
    "\n",
    "        self.W_r = np.random.uniform(-lim1,lim1,(self.input_dim,self.hidden_dim))\n",
    "        self.U_r = np.random.uniform(-lim1_hid,lim1_hid,(self.hidden_dim,self.hidden_dim))\n",
    "        self.B_r = np.random.uniform(-lim1,lim1,(1,self.hidden_dim))\n",
    "\n",
    "        self.W_h = np.random.uniform(-lim1,lim1,(self.input_dim,self.hidden_dim))\n",
    "        self.U_h = np.random.uniform(-lim1_hid,lim1_hid,(self.hidden_dim,self.hidden_dim))\n",
    "        self.B_h = np.random.uniform(-lim1,lim1,(1,self.hidden_dim))\n",
    "\n",
    "        \n",
    "        lim2 = Xavier(self.hidden_dim,self.output_class)\n",
    "        self.W = np.random.uniform(-lim2,lim2,(self.hidden_dim,self.output_class))\n",
    "        self.B = np.random.uniform(-lim2,lim2,(1,self.output_class))\n",
    "\n",
    "        # To keep track loss and accuracy score :     \n",
    "        self.train_loss, self.test_loss, self.train_acc, self.test_acc = [],[],[],[]\n",
    "        \n",
    "        # To keep previous updates in momentum :\n",
    "        self.previous_updates = [0] * 10\n",
    "        \n",
    "        # For AdaGrad:\n",
    "        self.cache = [0] * 11   \n",
    "        self.cache_rmsprop = [0] * 11\n",
    "        self.m = [0] * 11\n",
    "        self.v = [0] * 11\n",
    "        self.t = 1\n",
    "\n",
    "    def cell_forward(self,X,h_prev):\n",
    "        \"\"\"\n",
    "\n",
    "        Takes input, previous hidden state and previous cell state, compute:\n",
    "          * Forget gate + Input gate + New candidate input + New cell state + \n",
    "          * output gate + hidden state. Then, classify by softmax.\n",
    "        \"\"\"\n",
    "                      \n",
    "\n",
    "        # Update gate:\n",
    "        update_gate = activations.sigmoid(\n",
    "          np.dot(X, self.W_z) + np.dot(h_prev, self.U_z) + self.B_z\n",
    "        )\n",
    "       \n",
    "        # Reset gate:\n",
    "        reset_gate = activations.sigmoid(\n",
    "          np.dot(X, self.W_r) + np.dot(h_prev, self.U_r) + self.B_r\n",
    "        )\n",
    "\n",
    "        # Current memory content:\n",
    "        h_hat = np.tanh(\n",
    "          np.dot(X,self.W_h) + np.dot(np.multiply(reset_gate, h_prev), self.U_h) + self.B_h\n",
    "        )\n",
    "\n",
    "        # Hidden state:\n",
    "        hidden_state = np.multiply(update_gate,h_prev) + np.multiply((1 - update_gate), h_hat)\n",
    "\n",
    "\n",
    "        # Classifiers (Softmax) :\n",
    "        dense = np.dot(hidden_state, self.W) + self.B\n",
    "        probs = activations.softmax(dense, axis = -1)\n",
    "\n",
    "        return update_gate, reset_gate, h_hat, hidden_state, dense, probs\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, X, h_prev):\n",
    "        x_s,z_s,r_s,h_hat = {}, {},{}, {}\n",
    "        h_s = {}\n",
    "        y_s,p_s = {}, {}        \n",
    "\n",
    "        h_s[-1] = np.copy(h_prev)\n",
    "        \n",
    "\n",
    "        for t in range(self.seq_len):\n",
    "            x_s[t] = X[:,t,:]\n",
    "\n",
    "            z_s[t], r_s[t], h_hat[t], h_s[t], y_s[t], p_s[t] = self.cell_forward(\n",
    "              x_s[t], \n",
    "              h_s[t-1]\n",
    "            )\n",
    "\n",
    "        return x_s, z_s, r_s, h_hat, h_s, y_s, p_s\n",
    "    \n",
    "    def BPTT(self, outs, Y):\n",
    "\n",
    "        x_s,z_s, r_s, h_hat, h_s, y_s, p_s = outs\n",
    "\n",
    "        dW_z, dW_r,dW_h, dW = np.zeros_like(self.W_z), np.zeros_like(self.W_r), np.zeros_like(self.W_h),np.zeros_like(self.W)\n",
    "\n",
    "        dU_z, dU_r,dU_h, = np.zeros_like(self.U_z), np.zeros_like(self.U_r), np.zeros_like(self.U_h)\n",
    "\n",
    "\n",
    "        dB_z, dB_r,dB_h,dB = np.zeros_like(self.B_z), np.zeros_like(self.B_r),np.zeros_like(self.B_h),np.zeros_like(self.B)\n",
    "\n",
    "        dh_next = np.zeros_like(h_s[0]) \n",
    "        \n",
    "\n",
    "        # w.r.t. softmax input\n",
    "        ddense = np.copy(p_s[self.seq_len - 1])\n",
    "        ddense[np.arange(len(Y)), np.argmax(Y, axis=1)] -= 1\n",
    "        #ddense[np.argmax(Y,1)] -=1\n",
    "        #ddense = y_s[149] - Y\n",
    "        # Softmax classifier's :\n",
    "        dW = np.dot(h_s[self.seq_len - 1].T, ddense)\n",
    "        dB = np.sum(ddense, axis = 0, keepdims = True)\n",
    "\n",
    "        # Backprop through time:\n",
    "        for t in reversed(range(1,self.seq_len)):           \n",
    "                        \n",
    "            # w.r.t. softmax input\n",
    "            #ddense = np.copy(probs)\n",
    "            #ddense[np.arange(len(Y)),np.argmax(Y,1)] -= 1\n",
    "            #ddense[np.arange(len(Y)),np.argmax(Y,1)] -=1\n",
    "            # Softmax classifier's :\n",
    "            #dW += np.dot(hidden_state.T,ddense)\n",
    "            #dB += np.sum(ddense,axis = 0, keepdims = True)\n",
    "\n",
    "\n",
    "            # Curernt memort state :\n",
    "            dh = np.dot(ddense, self.W.T) + dh_next            \n",
    "            dh_hat = dh * (1-z_s[t])\n",
    "            dh_hat = dh_hat * dtanh(h_hat[t])\n",
    "            dW_h += np.dot(x_s[t].T, dh_hat)\n",
    "            dU_h += np.dot((r_s[t] * h_s[t-1]).T, dh_hat)\n",
    "            dB_h += np.sum(dh_hat, axis = 0, keepdims = True)\n",
    "\n",
    "            # Reset gate:\n",
    "            dr_1 = np.dot(dh_hat,self.U_h.T)\n",
    "            dr = dr_1  * h_s[t-1]\n",
    "            dr = dr * dsigmoid(r_s[t])\n",
    "            dW_r += np.dot(x_s[t].T,dr)\n",
    "            dU_r += np.dot(h_s[t-1].T, dr)\n",
    "            dB_r += np.sum(dr, axis = 0, keepdims = True)\n",
    "\n",
    "            # Forget gate:\n",
    "            dz = dh * (h_s[t-1] - h_hat[t])\n",
    "            dz = dz * dsigmoid(z_s[t])\n",
    "            dW_z += np.dot(x_s[t].T,dz)\n",
    "            dU_z += np.dot(h_s[t-1].T,dz)\n",
    "            dB_z += np.sum(dz, axis = 0, keepdims = True)\n",
    "\n",
    "\n",
    "            # Nexts:\n",
    "            dh_next = np.dot(dz, self.U_z.T) + (dh * z_s[t]) + (dr_1 * r_s[t]) + np.dot(dr, self.U_r.T)\n",
    "\n",
    "        # List of gradients :\n",
    "        grads = [dW, dB, dW_z, dU_z, dB_z, dW_r, dU_r, dB_r, dW_h, dU_h, dB_h]\n",
    "\n",
    "        # Clipping gradients anyway\n",
    "        for grad in grads:\n",
    "            np.clip(grad, -15, 15, out = grad)\n",
    "\n",
    "        return h_s[self.seq_len - 1], grads\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, X, Y, X_val, y_val, epochs = 50, optimizer = 'SGD', verbose = True, crossVal = False):\n",
    "        \"\"\"\n",
    "        Given the traning dataset,their labels and number of epochs\n",
    "        fitting the model, and measure the performance\n",
    "        by validating training dataset.\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            print(f'Epoch : {epoch + 1}')\n",
    "\n",
    "            perm = np.random.permutation(X.shape[0])   \n",
    "\n",
    "            h_prev = np.zeros((self.batch_size, self.hidden_dim))\n",
    "\n",
    "            for i in range(round(X.shape[0] / self.batch_size) - 1): \n",
    "          \n",
    "                batch_start  =  i * self.batch_size\n",
    "                batch_finish = (i+1) * self.batch_size                \n",
    "                index = perm[batch_start:batch_finish]\n",
    "                \n",
    "                # Feeding random indexes:\n",
    "                X_feed = X[index]    \n",
    "                y_feed = Y[index]\n",
    "               \n",
    "                # Forward + BPTT + SGD:\n",
    "                cache_train = self.forward(X_feed,h_prev)\n",
    "                h,grads = self.BPTT(cache_train, y_feed)\n",
    "\n",
    "                if optimizer == 'SGD':                                                                \n",
    "                  self.SGD(grads)\n",
    "\n",
    "                elif optimizer == 'AdaGrad' :\n",
    "                  self.AdaGrad(grads)\n",
    "\n",
    "                elif optimizer == 'RMSprop':\n",
    "                  self.RMSprop(grads)\n",
    "                \n",
    "                elif optimizer == 'VanillaAdam':\n",
    "                  self.VanillaAdam(grads)\n",
    "                else:\n",
    "                  self.Adam(grads)\n",
    "\n",
    "                # Hidden state -------> Previous hidden state\n",
    "                h_prev = h\n",
    "\n",
    "            # Training metrics calculations:\n",
    "            cross_loss_train = self.CategoricalCrossEntropy(y_feed, cache_train[6][self.seq_len - 1])\n",
    "            predictions_train = self.predict(X)\n",
    "            acc_train = accuracy(\n",
    "              np.argmax(Y, axis = 1), \n",
    "              predictions_train\n",
    "            )\n",
    "\n",
    "            # Validation metrics calculations:\n",
    "            test_prevs = np.zeros((X_val.shape[0], self.hidden_dim))\n",
    "            _,__,___,____,_____,______, probs_test = self.forward(X_val,test_prevs)\n",
    "            cross_loss_val = self.CategoricalCrossEntropy(y_val,probs_test[self.seq_len - 1])\n",
    "            predictions_val = np.argmax(probs_test[self.seq_len - 1], 1)\n",
    "            acc_val = accuracy(\n",
    "              np.argmax(y_val, axis = 1),\n",
    "              predictions_val\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Accuracy : {acc_train}\")\n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Training :  Loss     : {cross_loss_train}\")\n",
    "                print('______________________________________________________________________________________\\n')                         \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Accuracy : {acc_val}\")                                        \n",
    "                print(f\"[{epoch + 1}/{epochs}] ------> Testing  :  Loss     : {cross_loss_val}\")\n",
    "                print('______________________________________________________________________________________\\n')\n",
    "                \n",
    "            self.train_loss.append(cross_loss_train)              \n",
    "            self.test_loss.append(cross_loss_val) \n",
    "            self.train_acc.append(acc_train)              \n",
    "            self.test_acc.append(acc_val)\n",
    "      \n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\" Return all weights/biases in sequential order starting from end in list form. \"\"\"        \n",
    "        return [\n",
    "          self.W, self.B, self.W_z, \n",
    "          self.U_z, self.B_z, self.W_r,\n",
    "          self.U_r, self.B_r,self.W_h, \n",
    "          self.U_h, self.B_h\n",
    "        ]\n",
    "\n",
    "    def SGD(self,grads):\n",
    "      \"\"\"\n",
    "\n",
    "      Stochastic gradient descent with momentum on mini-batches.\n",
    "      \"\"\"\n",
    "      prevs = []\n",
    "      for param, grad,prev_update in zip(self.params(),grads,self.previous_updates):            \n",
    "          delta = self.learning_rate * grad - self.mom_coeff * prev_update\n",
    "          param -= delta \n",
    "          prevs.append(delta)\n",
    "\n",
    "      self.previous_updates = prevs       \n",
    "\n",
    "      self.learning_rate *= 0.99999   \n",
    "\n",
    "    \n",
    "    def AdaGrad(self, grads):\n",
    "      \"\"\" AdaGrad adaptive optimization algorithm. \"\"\"         \n",
    "\n",
    "      for i, (param, grad) in enumerate(zip(self.params(), grads)):\n",
    "        self.cache[i] += grad **2\n",
    "        param += -self.learning_rate * grad / (np.sqrt(self.cache[i]) + 1e-6)\n",
    "\n",
    "    def RMSprop(self, grads, decay_rate = 0.9):\n",
    "      \"\"\" RMSprop adaptive optimization algorithm \"\"\"\n",
    "\n",
    "      for i, (param, grad) in enumerate(zip(self.params(), grads)):\n",
    "        self.cache_rmsprop[i] = decay_rate * self.cache_rmsprop[i] + (1-decay_rate) * grad **2\n",
    "        param += - self.learning_rate * grad / (np.sqrt(self.cache_rmsprop[i])+ 1e-6)\n",
    "        \n",
    "\n",
    "\n",
    "    def VanillaAdam(self, grads, beta1 = 0.9, beta2 = 0.999):\n",
    "        \"\"\" Adam optimizer, but bias correction is not implemented \"\"\"\n",
    "      \n",
    "        for i, (param, grad)  in enumerate(zip(self.params(), grads)):\n",
    "          self.m[i] = beta1 * self.m[i] + (1-beta1) * grad          \n",
    "          self.v[i] = beta2 * self.v[i] + (1-beta2) * grad **2  \n",
    "          param += -self.learning_rate * self.m[i] / (np.sqrt(self.v[i]) + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "    def Adam(self, grads, beta1 = 0.9, beta2 = 0.999):\n",
    "        \"\"\" Adam optimizer, bias correction is implemented. \"\"\"\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(self.params(), grads)):\n",
    "          self.m[i] = beta1 * self.m[i] + (1-beta1) * grad          \n",
    "          self.v[i] = beta2 * self.v[i] + (1-beta2) * grad **2\n",
    "          m_corrected = self.m[i] / (1-beta1**self.t)\n",
    "          v_corrected = self.v[i] / (1-beta2**self.t)\n",
    "          param += -self.learning_rate * m_corrected / (np.sqrt(v_corrected) + 1e-8)\n",
    "\n",
    "        self.t +=1\n",
    "    \n",
    "    \n",
    "    def CategoricalCrossEntropy(self,labels,preds):\n",
    "        \"\"\"\n",
    "        Computes cross entropy between labels and model's predictions\n",
    "        \"\"\"\n",
    "        predictions = np.clip(preds, 1e-12, 1. - 1e-12)\n",
    "        N = predictions.shape[0]         \n",
    "        return -np.sum(labels * np.log(predictions + 1e-9)) / N\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Return predictions, (not one hot encoded format)\n",
    "        \"\"\"\n",
    "\n",
    "        # Give zeros to hidden/cell states:\n",
    "        pasts = np.zeros((X.shape[0], self.hidden_dim))\n",
    "        _,__,___,____,_____,______, probs = self.forward(X, pasts)\n",
    "        return np.argmax(probs[self.seq_len - 1], axis=1)\n",
    "\n",
    "    def history(self):\n",
    "        return {\n",
    "          'TrainLoss' : self.train_loss,\n",
    "          'TrainAcc'  : self.train_acc,\n",
    "          'TestLoss'  : self.test_loss,\n",
    "          'TestAcc'   : self.test_acc\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(path: str = 'glove.6B.50d.txt', save_path: str = '../word_embeddings/love.6B.50d') -> dict:\n",
    "    \"\"\" Given the path of the embedding file, return a dictionary of embedding\"\"\"\n",
    "    \n",
    "    assert os.path.exists(save_path), f\"The path {path} is not exists\"\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(path, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(\n",
    "            values[1:], \n",
    "            dtype='float32'\n",
    "        )\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    if save_path:\n",
    "        save_obj(\n",
    "            embeddings_index,\n",
    "            save_path\n",
    "        )\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810054, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/final_training_data.parquet')\n",
    "\n",
    "embeddings_index = load_obj(\n",
    "    os.path.join(\n",
    "        os.getcwd(), \n",
    "        '../word_embeddings/glove.6B.300d'\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.sample(int(1e5))\n",
    "\n",
    "train_set, val_set, test_set = train_test_split(\n",
    "    df, \n",
    "    split_size = [0.7, 0.1, 0.2],\n",
    "    random_state=51\n",
    ")\n",
    "\n",
    "training_texts = train_set['clean_freq_removed_text']\n",
    "validation_texts = val_set['clean_freq_removed_text']\n",
    "\n",
    "max_token_num = max_token_size(training_texts)\n",
    "\n",
    "embedding_transformer = PretrainedWord2Vec(\n",
    "    embeddings_index,\n",
    "    max_token_num\n",
    ")\n",
    "\n",
    "embedding_transformer.fit(training_texts)\n",
    "training_vectors = embedding_transformer.transform(training_texts)\n",
    "\n",
    "embedding_transformer = PretrainedWord2Vec(\n",
    "    embeddings_index,\n",
    "    max_token_num\n",
    ")\n",
    "embedding_transformer.fit(validation_texts)\n",
    "validation_vectors = embedding_transformer.transform(validation_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = Activations()\n",
    "encoder = OneHotEncoder() \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "seq_len, input_dim = training_vectors.shape[-2:]\n",
    "\n",
    "y_train = train_set['sentiment'].values\n",
    "y_train_hot  = encoder.fit_transform(y_train)\n",
    "\n",
    "y_val = val_set['sentiment'].values\n",
    "y_val_hot  = encoder.fit_transform(y_val)\n",
    "\n",
    "X_train = scaler.fit_transform(training_vectors)\n",
    "X_val = scaler.transform(validation_vectors)\n",
    "\n",
    "\n",
    "\n",
    "model = RNN(\n",
    "    input_dim = input_dim,\n",
    "    learning_rate = 1e-4,\n",
    "    seq_len = seq_len,\n",
    "    mom_coeff = 0.0,\n",
    "    hidden_dim = 64,\n",
    "    output_class = len(np.unique(y_train))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train_hot,\n",
    "    X_val,\n",
    "    y_val_hot,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(\n",
    "    input_dim=input_dim,\n",
    "    seq_len = seq_len,\n",
    "    learning_rate = 5e-3,\n",
    "    mom_coeff = 0.9,\n",
    "    batch_size = 32,\n",
    "    hidden_dim = 25,\n",
    "    output_class = len(np.unique(y))\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "[1/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[1/100] ------> Training :  Loss     : 0.006981591009452204\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[1/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[1/100] ------> Testing  :  Loss     : 2.6809616962904808\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 2\n",
      "[2/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[2/100] ------> Training :  Loss     : 0.0004372586468390703\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[2/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[2/100] ------> Testing  :  Loss     : 2.747327165471364\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 3\n",
      "[3/100] ------> Training :  Accuracy : 100.0\n",
      "[3/100] ------> Training :  Loss     : 0.0004769559953018033\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[3/100] ------> Testing  :  Accuracy : 60.0\n",
      "[3/100] ------> Testing  :  Loss     : 2.6027257308400995\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 4\n",
      "[4/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[4/100] ------> Training :  Loss     : 0.0037637658973568673\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[4/100] ------> Testing  :  Accuracy : 61.0\n",
      "[4/100] ------> Testing  :  Loss     : 2.322231551557719\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 5\n",
      "[5/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[5/100] ------> Training :  Loss     : 0.0021994913650918435\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[5/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[5/100] ------> Testing  :  Loss     : 2.502807742764344\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 6\n",
      "[6/100] ------> Training :  Accuracy : 100.0\n",
      "[6/100] ------> Training :  Loss     : 0.0005375374753351847\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[6/100] ------> Testing  :  Accuracy : 61.0\n",
      "[6/100] ------> Testing  :  Loss     : 2.638475792845775\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 7\n",
      "[7/100] ------> Training :  Accuracy : 100.0\n",
      "[7/100] ------> Training :  Loss     : 0.00044286627852489703\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[7/100] ------> Testing  :  Accuracy : 60.0\n",
      "[7/100] ------> Testing  :  Loss     : 2.7748525642094397\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 8\n",
      "[8/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[8/100] ------> Training :  Loss     : 0.00028495135318308703\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[8/100] ------> Testing  :  Accuracy : 59.0\n",
      "[8/100] ------> Testing  :  Loss     : 2.874428427312469\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 9\n",
      "[9/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[9/100] ------> Training :  Loss     : 0.0009659237198274611\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[9/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[9/100] ------> Testing  :  Loss     : 2.3893871524568695\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 10\n",
      "[10/100] ------> Training :  Accuracy : 100.0\n",
      "[10/100] ------> Training :  Loss     : 0.0008365527554346716\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[10/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[10/100] ------> Testing  :  Loss     : 2.3121673873180266\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 11\n",
      "[11/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[11/100] ------> Training :  Loss     : 0.0009683347755986087\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[11/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[11/100] ------> Testing  :  Loss     : 2.4950567333424285\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 12\n",
      "[12/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[12/100] ------> Training :  Loss     : 0.00034586346355292664\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[12/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[12/100] ------> Testing  :  Loss     : 2.8093984975263306\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 13\n",
      "[13/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[13/100] ------> Training :  Loss     : 0.0003420655480027102\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[13/100] ------> Testing  :  Accuracy : 54.0\n",
      "[13/100] ------> Testing  :  Loss     : 2.9909468412798814\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 14\n",
      "[14/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[14/100] ------> Training :  Loss     : 0.0004249475588322204\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[14/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[14/100] ------> Testing  :  Loss     : 2.8965162731233023\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 15\n",
      "[15/100] ------> Training :  Accuracy : 100.0\n",
      "[15/100] ------> Training :  Loss     : 0.000372762955322835\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[15/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[15/100] ------> Testing  :  Loss     : 3.015287638910437\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 16\n",
      "[16/100] ------> Training :  Accuracy : 98.57142857142858\n",
      "[16/100] ------> Training :  Loss     : 0.002431502361603101\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[16/100] ------> Testing  :  Accuracy : 54.0\n",
      "[16/100] ------> Testing  :  Loss     : 2.7254620259921523\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 17\n",
      "[17/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[17/100] ------> Training :  Loss     : 0.0011870257513009158\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[17/100] ------> Testing  :  Accuracy : 49.0\n",
      "[17/100] ------> Testing  :  Loss     : 2.911096491050531\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 18\n",
      "[18/100] ------> Training :  Accuracy : 99.14285714285714\n",
      "[18/100] ------> Training :  Loss     : 0.017353054308753252\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[18/100] ------> Testing  :  Accuracy : 54.0\n",
      "[18/100] ------> Testing  :  Loss     : 1.8630991306372602\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 19\n",
      "[19/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[19/100] ------> Training :  Loss     : 0.014234352127408648\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[19/100] ------> Testing  :  Accuracy : 54.0\n",
      "[19/100] ------> Testing  :  Loss     : 2.0169780131402026\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 20\n",
      "[20/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[20/100] ------> Training :  Loss     : 0.009417115806996279\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[20/100] ------> Testing  :  Accuracy : 52.0\n",
      "[20/100] ------> Testing  :  Loss     : 2.2148883583244743\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 21\n",
      "[21/100] ------> Training :  Accuracy : 100.0\n",
      "[21/100] ------> Training :  Loss     : 0.009073788706228153\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[21/100] ------> Testing  :  Accuracy : 53.0\n",
      "[21/100] ------> Testing  :  Loss     : 1.9236949754660881\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 22\n",
      "[22/100] ------> Training :  Accuracy : 100.0\n",
      "[22/100] ------> Training :  Loss     : 0.006148065101472974\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[22/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[22/100] ------> Testing  :  Loss     : 2.2419866761464964\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 23\n",
      "[23/100] ------> Training :  Accuracy : 99.42857142857143\n",
      "[23/100] ------> Training :  Loss     : 0.0016313986345206884\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[23/100] ------> Testing  :  Accuracy : 52.0\n",
      "[23/100] ------> Testing  :  Loss     : 2.838993914221861\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 24\n",
      "[24/100] ------> Training :  Accuracy : 100.0\n",
      "[24/100] ------> Training :  Loss     : 0.0006622830030762653\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[24/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[24/100] ------> Testing  :  Loss     : 2.550273759229592\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 25\n",
      "[25/100] ------> Training :  Accuracy : 100.0\n",
      "[25/100] ------> Training :  Loss     : 0.0004760374298531456\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[25/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[25/100] ------> Testing  :  Loss     : 2.64311468146773\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 26\n",
      "[26/100] ------> Training :  Accuracy : 100.0\n",
      "[26/100] ------> Training :  Loss     : 0.0003820127632255397\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[26/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[26/100] ------> Testing  :  Loss     : 2.746235411650801\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 27\n",
      "[27/100] ------> Training :  Accuracy : 100.0\n",
      "[27/100] ------> Training :  Loss     : 0.0008398862558425623\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[27/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[27/100] ------> Testing  :  Loss     : 2.5683059874061835\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 28\n",
      "[28/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[28/100] ------> Training :  Loss     : 0.0012523734417341594\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[28/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[28/100] ------> Testing  :  Loss     : 2.6042494277305\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 29\n",
      "[29/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[29/100] ------> Training :  Loss     : 0.0012677415982182824\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[29/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[29/100] ------> Testing  :  Loss     : 2.527867341855707\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 30\n",
      "[30/100] ------> Training :  Accuracy : 100.0\n",
      "[30/100] ------> Training :  Loss     : 0.0006946376105470946\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[30/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[30/100] ------> Testing  :  Loss     : 2.709328643691649\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 31\n",
      "[31/100] ------> Training :  Accuracy : 100.0\n",
      "[31/100] ------> Training :  Loss     : 0.0006969833496159531\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[31/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[31/100] ------> Testing  :  Loss     : 2.7859631438029693\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 32\n",
      "[32/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[32/100] ------> Training :  Loss     : 0.0017406121498659715\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[32/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[32/100] ------> Testing  :  Loss     : 2.724017074748569\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 33\n",
      "[33/100] ------> Training :  Accuracy : 100.0\n",
      "[33/100] ------> Training :  Loss     : 0.0009891978221133278\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[33/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[33/100] ------> Testing  :  Loss     : 2.678806854057599\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 34\n",
      "[34/100] ------> Training :  Accuracy : 100.0\n",
      "[34/100] ------> Training :  Loss     : 0.0009813014227910484\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[34/100] ------> Testing  :  Accuracy : 59.0\n",
      "[34/100] ------> Testing  :  Loss     : 2.6141671408324356\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 35\n",
      "[35/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[35/100] ------> Training :  Loss     : 0.00101430920003945\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[35/100] ------> Testing  :  Accuracy : 54.0\n",
      "[35/100] ------> Testing  :  Loss     : 2.4450434276537965\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 36\n",
      "[36/100] ------> Training :  Accuracy : 100.0\n",
      "[36/100] ------> Training :  Loss     : 0.0017216243633199151\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[36/100] ------> Testing  :  Accuracy : 60.0\n",
      "[36/100] ------> Testing  :  Loss     : 2.2338788882357608\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 37\n",
      "[37/100] ------> Training :  Accuracy : 100.0\n",
      "[37/100] ------> Training :  Loss     : 0.0005949208560068391\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[37/100] ------> Testing  :  Accuracy : 60.0\n",
      "[37/100] ------> Testing  :  Loss     : 2.426962984555736\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 38\n",
      "[38/100] ------> Training :  Accuracy : 100.0\n",
      "[38/100] ------> Training :  Loss     : 0.00041306099170697493\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[38/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[38/100] ------> Testing  :  Loss     : 2.5966157754406876\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 39\n",
      "[39/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[39/100] ------> Training :  Loss     : 0.003501360470738708\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[39/100] ------> Testing  :  Accuracy : 61.0\n",
      "[39/100] ------> Testing  :  Loss     : 2.5472562751980656\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 40\n",
      "[40/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[40/100] ------> Training :  Loss     : 0.0008357753510852574\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[40/100] ------> Testing  :  Accuracy : 61.0\n",
      "[40/100] ------> Testing  :  Loss     : 2.113755621023346\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 41\n",
      "[41/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[41/100] ------> Training :  Loss     : 0.0005460255618350532\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[41/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[41/100] ------> Testing  :  Loss     : 2.6921513999730315\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 42\n",
      "[42/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[42/100] ------> Training :  Loss     : 0.0007602038596338466\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[42/100] ------> Testing  :  Accuracy : 62.0\n",
      "[42/100] ------> Testing  :  Loss     : 2.5315026112304855\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 43\n",
      "[43/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[43/100] ------> Training :  Loss     : 0.027261414707557614\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[43/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[43/100] ------> Testing  :  Loss     : 2.337345654602798\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 44\n",
      "[44/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[44/100] ------> Training :  Loss     : 0.0005446190298615909\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[44/100] ------> Testing  :  Accuracy : 61.0\n",
      "[44/100] ------> Testing  :  Loss     : 2.598625405919744\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 45\n",
      "[45/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[45/100] ------> Training :  Loss     : 0.008807539321724029\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[45/100] ------> Testing  :  Accuracy : 52.0\n",
      "[45/100] ------> Testing  :  Loss     : 2.1068641509033825\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 46\n",
      "[46/100] ------> Training :  Accuracy : 100.0\n",
      "[46/100] ------> Training :  Loss     : 0.0010263207256333746\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[46/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[46/100] ------> Testing  :  Loss     : 2.53774023471246\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 47\n",
      "[47/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[47/100] ------> Training :  Loss     : 0.000374821918804036\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[47/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[47/100] ------> Testing  :  Loss     : 2.9058599412483317\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 48\n",
      "[48/100] ------> Training :  Accuracy : 100.0\n",
      "[48/100] ------> Training :  Loss     : 0.02796587212783413\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[48/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[48/100] ------> Testing  :  Loss     : 2.8279083393658975\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 49\n",
      "[49/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[49/100] ------> Training :  Loss     : 0.013053122149392096\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[49/100] ------> Testing  :  Accuracy : 60.0\n",
      "[49/100] ------> Testing  :  Loss     : 2.0220492926514226\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 50\n",
      "[50/100] ------> Training :  Accuracy : 100.0\n",
      "[50/100] ------> Training :  Loss     : 0.0006217092136725232\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[50/100] ------> Testing  :  Accuracy : 59.0\n",
      "[50/100] ------> Testing  :  Loss     : 2.204944977224129\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 51\n",
      "[51/100] ------> Training :  Accuracy : 100.0\n",
      "[51/100] ------> Training :  Loss     : 0.0005384362537293\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[51/100] ------> Testing  :  Accuracy : 59.0\n",
      "[51/100] ------> Testing  :  Loss     : 2.646357848726051\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 52\n",
      "[52/100] ------> Training :  Accuracy : 100.0\n",
      "[52/100] ------> Training :  Loss     : 0.0014483070470900786\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[52/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[52/100] ------> Testing  :  Loss     : 2.138722204510757\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 53\n",
      "[53/100] ------> Training :  Accuracy : 100.0\n",
      "[53/100] ------> Training :  Loss     : 0.0009036537536643154\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[53/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[53/100] ------> Testing  :  Loss     : 2.459093667784973\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 54\n",
      "[54/100] ------> Training :  Accuracy : 100.0\n",
      "[54/100] ------> Training :  Loss     : 0.00044897700950071915\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[54/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[54/100] ------> Testing  :  Loss     : 2.7840723159881704\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 55\n",
      "[55/100] ------> Training :  Accuracy : 100.0\n",
      "[55/100] ------> Training :  Loss     : 0.00038119078653831354\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[55/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[55/100] ------> Testing  :  Loss     : 2.9835517473241975\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 56\n",
      "[56/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[56/100] ------> Training :  Loss     : 0.0003065919036772785\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[56/100] ------> Testing  :  Accuracy : 54.0\n",
      "[56/100] ------> Testing  :  Loss     : 3.112533881224907\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 57\n",
      "[57/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[57/100] ------> Training :  Loss     : 0.00022155098485556634\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[57/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[57/100] ------> Testing  :  Loss     : 3.103477248493887\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 58\n",
      "[58/100] ------> Training :  Accuracy : 99.42857142857143\n",
      "[58/100] ------> Training :  Loss     : 0.005411399362848912\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[58/100] ------> Testing  :  Accuracy : 54.0\n",
      "[58/100] ------> Testing  :  Loss     : 2.0710436712366103\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 59\n",
      "[59/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[59/100] ------> Training :  Loss     : 0.01087688272396074\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[59/100] ------> Testing  :  Accuracy : 61.0\n",
      "[59/100] ------> Testing  :  Loss     : 2.2094454723626984\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 60\n",
      "[60/100] ------> Training :  Accuracy : 100.0\n",
      "[60/100] ------> Training :  Loss     : 0.0006232748056696906\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[60/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[60/100] ------> Testing  :  Loss     : 2.5580527905869803\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 61\n",
      "[61/100] ------> Training :  Accuracy : 100.0\n",
      "[61/100] ------> Training :  Loss     : 0.00039021079621111147\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[61/100] ------> Testing  :  Accuracy : 59.0\n",
      "[61/100] ------> Testing  :  Loss     : 2.724182817101432\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 62\n",
      "[62/100] ------> Training :  Accuracy : 100.0\n",
      "[62/100] ------> Training :  Loss     : 0.0003220296208096681\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[62/100] ------> Testing  :  Accuracy : 59.0\n",
      "[62/100] ------> Testing  :  Loss     : 2.825297019222023\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 63\n",
      "[63/100] ------> Training :  Accuracy : 100.0\n",
      "[63/100] ------> Training :  Loss     : 0.00024193413937692113\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[63/100] ------> Testing  :  Accuracy : 59.0\n",
      "[63/100] ------> Testing  :  Loss     : 2.8973718914650277\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 64\n",
      "[64/100] ------> Training :  Accuracy : 100.0\n",
      "[64/100] ------> Training :  Loss     : 0.0003293609826310246\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[64/100] ------> Testing  :  Accuracy : 59.0\n",
      "[64/100] ------> Testing  :  Loss     : 2.9534312179471183\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 65\n",
      "[65/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[65/100] ------> Training :  Loss     : 0.00032927096638723824\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[65/100] ------> Testing  :  Accuracy : 60.0\n",
      "[65/100] ------> Testing  :  Loss     : 2.960110658214483\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 66\n",
      "[66/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[66/100] ------> Training :  Loss     : 0.07856888511426635\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[66/100] ------> Testing  :  Accuracy : 61.0\n",
      "[66/100] ------> Testing  :  Loss     : 2.2236416614911096\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 67\n",
      "[67/100] ------> Training :  Accuracy : 100.0\n",
      "[67/100] ------> Training :  Loss     : 0.04078007643144169\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[67/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[67/100] ------> Testing  :  Loss     : 1.8973610332456101\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 68\n",
      "[68/100] ------> Training :  Accuracy : 100.0\n",
      "[68/100] ------> Training :  Loss     : 0.0031102672491584403\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[68/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[68/100] ------> Testing  :  Loss     : 2.2369714084661063\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 69\n",
      "[69/100] ------> Training :  Accuracy : 100.0\n",
      "[69/100] ------> Training :  Loss     : 0.0010643793185473784\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[69/100] ------> Testing  :  Accuracy : 63.0\n",
      "[69/100] ------> Testing  :  Loss     : 2.2783834660541036\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 70\n",
      "[70/100] ------> Training :  Accuracy : 100.0\n",
      "[70/100] ------> Training :  Loss     : 0.001036224352241241\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[70/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[70/100] ------> Testing  :  Loss     : 2.171133452207844\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 71\n",
      "[71/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[71/100] ------> Training :  Loss     : 0.00069939022829026\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[71/100] ------> Testing  :  Accuracy : 64.0\n",
      "[71/100] ------> Testing  :  Loss     : 2.3320852121245004\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 72\n",
      "[72/100] ------> Training :  Accuracy : 100.0\n",
      "[72/100] ------> Training :  Loss     : 0.001870646124439661\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[72/100] ------> Testing  :  Accuracy : 62.0\n",
      "[72/100] ------> Testing  :  Loss     : 2.294716490818839\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 73\n",
      "[73/100] ------> Training :  Accuracy : 100.0\n",
      "[73/100] ------> Training :  Loss     : 0.0005323011120582536\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[73/100] ------> Testing  :  Accuracy : 61.0\n",
      "[73/100] ------> Testing  :  Loss     : 2.4732425316276845\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 74\n",
      "[74/100] ------> Training :  Accuracy : 100.0\n",
      "[74/100] ------> Training :  Loss     : 0.00036102675213449344\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[74/100] ------> Testing  :  Accuracy : 60.0\n",
      "[74/100] ------> Testing  :  Loss     : 2.606123641534411\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 75\n",
      "[75/100] ------> Training :  Accuracy : 100.0\n",
      "[75/100] ------> Training :  Loss     : 0.00024075764191572915\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[75/100] ------> Testing  :  Accuracy : 60.0\n",
      "[75/100] ------> Testing  :  Loss     : 2.679843116987951\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 76\n",
      "[76/100] ------> Training :  Accuracy : 100.0\n",
      "[76/100] ------> Training :  Loss     : 0.00023152927785491888\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[76/100] ------> Testing  :  Accuracy : 60.0\n",
      "[76/100] ------> Testing  :  Loss     : 2.7623482282866236\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 77\n",
      "[77/100] ------> Training :  Accuracy : 100.0\n",
      "[77/100] ------> Training :  Loss     : 0.0002043212354044817\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[77/100] ------> Testing  :  Accuracy : 61.0\n",
      "[77/100] ------> Testing  :  Loss     : 2.822447289469268\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 78\n",
      "[78/100] ------> Training :  Accuracy : 100.0\n",
      "[78/100] ------> Training :  Loss     : 0.00025534451654736\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[78/100] ------> Testing  :  Accuracy : 62.0\n",
      "[78/100] ------> Testing  :  Loss     : 2.8719273948245827\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 79\n",
      "[79/100] ------> Training :  Accuracy : 100.0\n",
      "[79/100] ------> Training :  Loss     : 0.0001687955125224531\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[79/100] ------> Testing  :  Accuracy : 62.0\n",
      "[79/100] ------> Testing  :  Loss     : 2.919368974789186\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 80\n",
      "[80/100] ------> Training :  Accuracy : 100.0\n",
      "[80/100] ------> Training :  Loss     : 0.00016006382445070405\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[80/100] ------> Testing  :  Accuracy : 62.0\n",
      "[80/100] ------> Testing  :  Loss     : 2.955340733177498\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 81\n",
      "[81/100] ------> Training :  Accuracy : 100.0\n",
      "[81/100] ------> Training :  Loss     : 0.00014653408206068315\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[81/100] ------> Testing  :  Accuracy : 62.0\n",
      "[81/100] ------> Testing  :  Loss     : 2.98652339094252\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 82\n",
      "[82/100] ------> Training :  Accuracy : 100.0\n",
      "[82/100] ------> Training :  Loss     : 0.00012685666318494802\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[82/100] ------> Testing  :  Accuracy : 62.0\n",
      "[82/100] ------> Testing  :  Loss     : 3.0152872122345413\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 83\n",
      "[83/100] ------> Training :  Accuracy : 100.0\n",
      "[83/100] ------> Training :  Loss     : 0.00011454976365729231\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[83/100] ------> Testing  :  Accuracy : 62.0\n",
      "[83/100] ------> Testing  :  Loss     : 3.043087418148277\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 84\n",
      "[84/100] ------> Training :  Accuracy : 100.0\n",
      "[84/100] ------> Training :  Loss     : 0.00012064916818094008\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[84/100] ------> Testing  :  Accuracy : 62.0\n",
      "[84/100] ------> Testing  :  Loss     : 3.06879712837998\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 85\n",
      "[85/100] ------> Training :  Accuracy : 100.0\n",
      "[85/100] ------> Training :  Loss     : 9.329599902277352e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[85/100] ------> Testing  :  Accuracy : 62.0\n",
      "[85/100] ------> Testing  :  Loss     : 3.0929525613115216\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 86\n",
      "[86/100] ------> Training :  Accuracy : 100.0\n",
      "[86/100] ------> Training :  Loss     : 0.00016172247370245383\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[86/100] ------> Testing  :  Accuracy : 62.0\n",
      "[86/100] ------> Testing  :  Loss     : 3.116443849992951\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 87\n",
      "[87/100] ------> Training :  Accuracy : 100.0\n",
      "[87/100] ------> Training :  Loss     : 9.079768828712518e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[87/100] ------> Testing  :  Accuracy : 62.0\n",
      "[87/100] ------> Testing  :  Loss     : 3.137538746845447\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 88\n",
      "[88/100] ------> Training :  Accuracy : 100.0\n",
      "[88/100] ------> Training :  Loss     : 7.400757456308077e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[88/100] ------> Testing  :  Accuracy : 62.0\n",
      "[88/100] ------> Testing  :  Loss     : 3.158602390218206\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 89\n",
      "[89/100] ------> Training :  Accuracy : 100.0\n",
      "[89/100] ------> Training :  Loss     : 8.076661856956335e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[89/100] ------> Testing  :  Accuracy : 62.0\n",
      "[89/100] ------> Testing  :  Loss     : 3.178597473390653\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 90\n",
      "[90/100] ------> Training :  Accuracy : 100.0\n",
      "[90/100] ------> Training :  Loss     : 7.18831876025745e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[90/100] ------> Testing  :  Accuracy : 62.0\n",
      "[90/100] ------> Testing  :  Loss     : 3.1974650179296913\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 91\n",
      "[91/100] ------> Training :  Accuracy : 100.0\n",
      "[91/100] ------> Training :  Loss     : 6.513513429778015e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[91/100] ------> Testing  :  Accuracy : 62.0\n",
      "[91/100] ------> Testing  :  Loss     : 3.215679898501878\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 92\n",
      "[92/100] ------> Training :  Accuracy : 100.0\n",
      "[92/100] ------> Training :  Loss     : 6.108780844776208e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[92/100] ------> Testing  :  Accuracy : 62.0\n",
      "[92/100] ------> Testing  :  Loss     : 3.2341136185457744\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 93\n",
      "[93/100] ------> Training :  Accuracy : 100.0\n",
      "[93/100] ------> Training :  Loss     : 6.144965024250033e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[93/100] ------> Testing  :  Accuracy : 61.0\n",
      "[93/100] ------> Testing  :  Loss     : 3.2445508152571496\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 94\n",
      "[94/100] ------> Training :  Accuracy : 100.0\n",
      "[94/100] ------> Training :  Loss     : 6.347790975185099e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[94/100] ------> Testing  :  Accuracy : 61.0\n",
      "[94/100] ------> Testing  :  Loss     : 3.2574601036574777\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 95\n",
      "[95/100] ------> Training :  Accuracy : 100.0\n",
      "[95/100] ------> Training :  Loss     : 6.739791274093792e-05\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[95/100] ------> Testing  :  Accuracy : 61.0\n",
      "[95/100] ------> Testing  :  Loss     : 3.2708197911982815\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 96\n",
      "[96/100] ------> Training :  Accuracy : 100.0\n",
      "[96/100] ------> Training :  Loss     : 0.00015342899462721237\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[96/100] ------> Testing  :  Accuracy : 60.0\n",
      "[96/100] ------> Testing  :  Loss     : 2.595201304984122\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 97\n",
      "[97/100] ------> Training :  Accuracy : 100.0\n",
      "[97/100] ------> Training :  Loss     : 0.0003952390106768293\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[97/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[97/100] ------> Testing  :  Loss     : 2.5258697484418313\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 98\n",
      "[98/100] ------> Training :  Accuracy : 100.0\n",
      "[98/100] ------> Training :  Loss     : 0.0004922825649061349\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[98/100] ------> Testing  :  Accuracy : 60.0\n",
      "[98/100] ------> Testing  :  Loss     : 2.4912935416867557\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 99\n",
      "[99/100] ------> Training :  Accuracy : 100.0\n",
      "[99/100] ------> Training :  Loss     : 0.00021002644357801298\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[99/100] ------> Testing  :  Accuracy : 59.0\n",
      "[99/100] ------> Testing  :  Loss     : 2.5820340849362515\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 100\n",
      "[100/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[100/100] ------> Training :  Loss     : 0.003301782267297876\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[100/100] ------> Testing  :  Accuracy : 62.0\n",
      "[100/100] ------> Testing  :  Loss     : 2.0957848171769466\n",
      "______________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm.fit(\n",
    "    X_train,\n",
    "    y_train_hot,\n",
    "    X_val,\n",
    "    y_val_hot,\n",
    "    epochs=100,\n",
    "    optimizer='RMSProb'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "[1/100] ------> Training :  Accuracy : 54.0\n",
      "[1/100] ------> Training :  Loss     : 0.688656936242345\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[1/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[1/100] ------> Testing  :  Loss     : 0.6868956270276857\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 2\n",
      "[2/100] ------> Training :  Accuracy : 67.57142857142857\n",
      "[2/100] ------> Training :  Loss     : 0.8024141304371601\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[2/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[2/100] ------> Testing  :  Loss     : 0.6982049093908129\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 3\n",
      "[3/100] ------> Training :  Accuracy : 74.14285714285714\n",
      "[3/100] ------> Training :  Loss     : 0.6290893163143847\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[3/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[3/100] ------> Testing  :  Loss     : 0.6863518391123986\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 4\n",
      "[4/100] ------> Training :  Accuracy : 81.14285714285714\n",
      "[4/100] ------> Training :  Loss     : 0.6041450102967284\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[4/100] ------> Testing  :  Accuracy : 53.0\n",
      "[4/100] ------> Testing  :  Loss     : 0.7123102202346862\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 5\n",
      "[5/100] ------> Training :  Accuracy : 85.0\n",
      "[5/100] ------> Training :  Loss     : 0.3655245280374123\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[5/100] ------> Testing  :  Accuracy : 59.0\n",
      "[5/100] ------> Testing  :  Loss     : 0.7439000785546395\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 6\n",
      "[6/100] ------> Training :  Accuracy : 86.42857142857143\n",
      "[6/100] ------> Training :  Loss     : 0.34753183221173944\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[6/100] ------> Testing  :  Accuracy : 62.0\n",
      "[6/100] ------> Testing  :  Loss     : 0.7676862711825921\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 7\n",
      "[7/100] ------> Training :  Accuracy : 90.42857142857143\n",
      "[7/100] ------> Training :  Loss     : 0.43775657069080465\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[7/100] ------> Testing  :  Accuracy : 59.0\n",
      "[7/100] ------> Testing  :  Loss     : 0.8505216489368322\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 8\n",
      "[8/100] ------> Training :  Accuracy : 93.42857142857143\n",
      "[8/100] ------> Training :  Loss     : 0.38715745154208236\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[8/100] ------> Testing  :  Accuracy : 59.0\n",
      "[8/100] ------> Testing  :  Loss     : 1.0899395538280792\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 9\n",
      "[9/100] ------> Training :  Accuracy : 95.28571428571428\n",
      "[9/100] ------> Training :  Loss     : 0.2741801229309937\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[9/100] ------> Testing  :  Accuracy : 60.0\n",
      "[9/100] ------> Testing  :  Loss     : 1.121464859581112\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 10\n",
      "[10/100] ------> Training :  Accuracy : 96.0\n",
      "[10/100] ------> Training :  Loss     : 0.33000484036433775\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[10/100] ------> Testing  :  Accuracy : 61.0\n",
      "[10/100] ------> Testing  :  Loss     : 1.2317788346867027\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 11\n",
      "[11/100] ------> Training :  Accuracy : 97.57142857142857\n",
      "[11/100] ------> Training :  Loss     : 0.03762536336423317\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[11/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[11/100] ------> Testing  :  Loss     : 1.2154633464800542\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 12\n",
      "[12/100] ------> Training :  Accuracy : 98.85714285714286\n",
      "[12/100] ------> Training :  Loss     : 0.07938822239397256\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[12/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[12/100] ------> Testing  :  Loss     : 1.4268007252585901\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 13\n",
      "[13/100] ------> Training :  Accuracy : 97.42857142857143\n",
      "[13/100] ------> Training :  Loss     : 0.16408592729985838\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[13/100] ------> Testing  :  Accuracy : 60.0\n",
      "[13/100] ------> Testing  :  Loss     : 1.030783254106416\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 14\n",
      "[14/100] ------> Training :  Accuracy : 97.14285714285714\n",
      "[14/100] ------> Training :  Loss     : 0.05543038863235591\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[14/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[14/100] ------> Testing  :  Loss     : 1.5958699924340038\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 15\n",
      "[15/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[15/100] ------> Training :  Loss     : 0.05224389055148698\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[15/100] ------> Testing  :  Accuracy : 63.0\n",
      "[15/100] ------> Testing  :  Loss     : 1.3581409913980138\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 16\n",
      "[16/100] ------> Training :  Accuracy : 97.42857142857143\n",
      "[16/100] ------> Training :  Loss     : 0.10404144535425203\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[16/100] ------> Testing  :  Accuracy : 62.0\n",
      "[16/100] ------> Testing  :  Loss     : 1.3311398958232459\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 17\n",
      "[17/100] ------> Training :  Accuracy : 98.42857142857143\n",
      "[17/100] ------> Training :  Loss     : 0.08203812137493181\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[17/100] ------> Testing  :  Accuracy : 60.0\n",
      "[17/100] ------> Testing  :  Loss     : 1.4029031911312992\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 18\n",
      "[18/100] ------> Training :  Accuracy : 98.42857142857143\n",
      "[18/100] ------> Training :  Loss     : 0.15611060021963885\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[18/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[18/100] ------> Testing  :  Loss     : 1.4310405282390755\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 19\n",
      "[19/100] ------> Training :  Accuracy : 97.28571428571429\n",
      "[19/100] ------> Training :  Loss     : 0.12076560277644749\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[19/100] ------> Testing  :  Accuracy : 60.0\n",
      "[19/100] ------> Testing  :  Loss     : 1.6467618277310336\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 20\n",
      "[20/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[20/100] ------> Training :  Loss     : 0.06067904849982372\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[20/100] ------> Testing  :  Accuracy : 61.0\n",
      "[20/100] ------> Testing  :  Loss     : 1.5462807636919798\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 21\n",
      "[21/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[21/100] ------> Training :  Loss     : 0.08896582988883332\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[21/100] ------> Testing  :  Accuracy : 60.0\n",
      "[21/100] ------> Testing  :  Loss     : 1.9569795246472355\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 22\n",
      "[22/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[22/100] ------> Training :  Loss     : 0.019470958312134645\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[22/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[22/100] ------> Testing  :  Loss     : 1.94254173234063\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 23\n",
      "[23/100] ------> Training :  Accuracy : 99.42857142857143\n",
      "[23/100] ------> Training :  Loss     : 0.010282049031755005\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[23/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[23/100] ------> Testing  :  Loss     : 1.7459231162710385\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 24\n",
      "[24/100] ------> Training :  Accuracy : 100.0\n",
      "[24/100] ------> Training :  Loss     : 0.020680836134076334\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[24/100] ------> Testing  :  Accuracy : 62.0\n",
      "[24/100] ------> Testing  :  Loss     : 1.9119122755383984\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 25\n",
      "[25/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[25/100] ------> Training :  Loss     : 0.032488693107103304\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[25/100] ------> Testing  :  Accuracy : 60.0\n",
      "[25/100] ------> Testing  :  Loss     : 2.075394394773997\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 26\n",
      "[26/100] ------> Training :  Accuracy : 98.57142857142858\n",
      "[26/100] ------> Training :  Loss     : 0.012463210259085387\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[26/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[26/100] ------> Testing  :  Loss     : 1.92192861146365\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 27\n",
      "[27/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[27/100] ------> Training :  Loss     : 0.007900759162945994\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[27/100] ------> Testing  :  Accuracy : 62.0\n",
      "[27/100] ------> Testing  :  Loss     : 1.8152904983273856\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 28\n",
      "[28/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[28/100] ------> Training :  Loss     : 0.08620126639131913\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[28/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[28/100] ------> Testing  :  Loss     : 2.1030048381216506\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 29\n",
      "[29/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[29/100] ------> Training :  Loss     : 0.015527082034827725\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[29/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[29/100] ------> Testing  :  Loss     : 1.9710318234139572\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 30\n",
      "[30/100] ------> Training :  Accuracy : 98.85714285714286\n",
      "[30/100] ------> Training :  Loss     : 0.006855549705489478\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[30/100] ------> Testing  :  Accuracy : 52.0\n",
      "[30/100] ------> Testing  :  Loss     : 1.9740337608821148\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 31\n",
      "[31/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[31/100] ------> Training :  Loss     : 0.0019975143992008197\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[31/100] ------> Testing  :  Accuracy : 61.0\n",
      "[31/100] ------> Testing  :  Loss     : 1.9636599786900197\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 32\n",
      "[32/100] ------> Training :  Accuracy : 100.0\n",
      "[32/100] ------> Training :  Loss     : 0.0010999080444802065\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[32/100] ------> Testing  :  Accuracy : 60.0\n",
      "[32/100] ------> Testing  :  Loss     : 2.250918612978594\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 33\n",
      "[33/100] ------> Training :  Accuracy : 99.42857142857143\n",
      "[33/100] ------> Training :  Loss     : 0.0027228300328662578\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[33/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[33/100] ------> Testing  :  Loss     : 2.384689331876722\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 34\n",
      "[34/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[34/100] ------> Training :  Loss     : 0.1418934077661897\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[34/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[34/100] ------> Testing  :  Loss     : 2.279015522719439\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 35\n",
      "[35/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[35/100] ------> Training :  Loss     : 0.18556714350094133\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[35/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[35/100] ------> Testing  :  Loss     : 2.16978847208052\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 36\n",
      "[36/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[36/100] ------> Training :  Loss     : 0.01223053249587809\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[36/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[36/100] ------> Testing  :  Loss     : 1.9104915001478835\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 37\n",
      "[37/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[37/100] ------> Training :  Loss     : 0.08908737284460404\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[37/100] ------> Testing  :  Accuracy : 59.0\n",
      "[37/100] ------> Testing  :  Loss     : 2.198805334070414\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 38\n",
      "[38/100] ------> Training :  Accuracy : 100.0\n",
      "[38/100] ------> Training :  Loss     : 0.005908303608320398\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[38/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[38/100] ------> Testing  :  Loss     : 2.3760820174667376\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 39\n",
      "[39/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[39/100] ------> Training :  Loss     : 0.0029158065807076018\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[39/100] ------> Testing  :  Accuracy : 55.00000000000001\n",
      "[39/100] ------> Testing  :  Loss     : 2.2918809431147054\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 40\n",
      "[40/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[40/100] ------> Training :  Loss     : 0.0022273411881100933\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[40/100] ------> Testing  :  Accuracy : 59.0\n",
      "[40/100] ------> Testing  :  Loss     : 2.270985150372884\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 41\n",
      "[41/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[41/100] ------> Training :  Loss     : 0.053091422368249624\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[41/100] ------> Testing  :  Accuracy : 62.0\n",
      "[41/100] ------> Testing  :  Loss     : 1.967302498762562\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 42\n",
      "[42/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[42/100] ------> Training :  Loss     : 0.07629926073282121\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[42/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[42/100] ------> Testing  :  Loss     : 2.2187106548903985\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 43\n",
      "[43/100] ------> Training :  Accuracy : 98.14285714285714\n",
      "[43/100] ------> Training :  Loss     : 0.0038015417652850683\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[43/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[43/100] ------> Testing  :  Loss     : 2.0634401074909334\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 44\n",
      "[44/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[44/100] ------> Training :  Loss     : 0.01030502759495567\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[44/100] ------> Testing  :  Accuracy : 52.0\n",
      "[44/100] ------> Testing  :  Loss     : 2.0996825904679692\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 45\n",
      "[45/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[45/100] ------> Training :  Loss     : 0.11425791647138651\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[45/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[45/100] ------> Testing  :  Loss     : 1.748407884415531\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 46\n",
      "[46/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[46/100] ------> Training :  Loss     : 0.004618571421309805\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[46/100] ------> Testing  :  Accuracy : 61.0\n",
      "[46/100] ------> Testing  :  Loss     : 1.899693220595989\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 47\n",
      "[47/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[47/100] ------> Training :  Loss     : 0.007352240172108589\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[47/100] ------> Testing  :  Accuracy : 62.0\n",
      "[47/100] ------> Testing  :  Loss     : 1.7720172932659601\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 48\n",
      "[48/100] ------> Training :  Accuracy : 100.0\n",
      "[48/100] ------> Training :  Loss     : 0.0022405332333528755\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[48/100] ------> Testing  :  Accuracy : 61.0\n",
      "[48/100] ------> Testing  :  Loss     : 1.9635027859410619\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 49\n",
      "[49/100] ------> Training :  Accuracy : 100.0\n",
      "[49/100] ------> Training :  Loss     : 0.006810215517531175\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[49/100] ------> Testing  :  Accuracy : 63.0\n",
      "[49/100] ------> Testing  :  Loss     : 1.4862611217477057\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 50\n",
      "[50/100] ------> Training :  Accuracy : 100.0\n",
      "[50/100] ------> Training :  Loss     : 0.008289794321503568\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[50/100] ------> Testing  :  Accuracy : 62.0\n",
      "[50/100] ------> Testing  :  Loss     : 1.675967119673798\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 51\n",
      "[51/100] ------> Training :  Accuracy : 100.0\n",
      "[51/100] ------> Training :  Loss     : 0.0012054363108865021\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[51/100] ------> Testing  :  Accuracy : 60.0\n",
      "[51/100] ------> Testing  :  Loss     : 1.9593941731935087\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 52\n",
      "[52/100] ------> Training :  Accuracy : 100.0\n",
      "[52/100] ------> Training :  Loss     : 0.0008809979298736283\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[52/100] ------> Testing  :  Accuracy : 61.0\n",
      "[52/100] ------> Testing  :  Loss     : 2.1862705808714087\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 53\n",
      "[53/100] ------> Training :  Accuracy : 100.0\n",
      "[53/100] ------> Training :  Loss     : 0.0009595270389995319\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[53/100] ------> Testing  :  Accuracy : 63.0\n",
      "[53/100] ------> Testing  :  Loss     : 2.187563343549674\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 54\n",
      "[54/100] ------> Training :  Accuracy : 100.0\n",
      "[54/100] ------> Training :  Loss     : 0.053319446624083715\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[54/100] ------> Testing  :  Accuracy : 62.0\n",
      "[54/100] ------> Testing  :  Loss     : 2.2702044810653423\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 55\n",
      "[55/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[55/100] ------> Training :  Loss     : 0.003959291068132333\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[55/100] ------> Testing  :  Accuracy : 59.0\n",
      "[55/100] ------> Testing  :  Loss     : 2.413665689712932\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 56\n",
      "[56/100] ------> Training :  Accuracy : 100.0\n",
      "[56/100] ------> Training :  Loss     : 0.004303079764456973\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[56/100] ------> Testing  :  Accuracy : 60.0\n",
      "[56/100] ------> Testing  :  Loss     : 2.0092150904661596\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 57\n",
      "[57/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[57/100] ------> Training :  Loss     : 0.005785959914589108\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[57/100] ------> Testing  :  Accuracy : 56.00000000000001\n",
      "[57/100] ------> Testing  :  Loss     : 2.300049861593274\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 58\n",
      "[58/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[58/100] ------> Training :  Loss     : 0.0014316435807406125\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[58/100] ------> Testing  :  Accuracy : 61.0\n",
      "[58/100] ------> Testing  :  Loss     : 2.1301958724927634\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 59\n",
      "[59/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[59/100] ------> Training :  Loss     : 0.019034888820245618\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[59/100] ------> Testing  :  Accuracy : 63.0\n",
      "[59/100] ------> Testing  :  Loss     : 1.81533164099184\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 60\n",
      "[60/100] ------> Training :  Accuracy : 100.0\n",
      "[60/100] ------> Training :  Loss     : 0.0015881060177345722\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[60/100] ------> Testing  :  Accuracy : 60.0\n",
      "[60/100] ------> Testing  :  Loss     : 1.933717025983982\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 61\n",
      "[61/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[61/100] ------> Training :  Loss     : 0.07467825213829954\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[61/100] ------> Testing  :  Accuracy : 59.0\n",
      "[61/100] ------> Testing  :  Loss     : 2.082478931324968\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 62\n",
      "[62/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[62/100] ------> Training :  Loss     : 0.0021858553824070917\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[62/100] ------> Testing  :  Accuracy : 56.99999999999999\n",
      "[62/100] ------> Testing  :  Loss     : 2.250089730360528\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 63\n",
      "[63/100] ------> Training :  Accuracy : 100.0\n",
      "[63/100] ------> Training :  Loss     : 0.005024770127095619\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[63/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[63/100] ------> Testing  :  Loss     : 2.184626602241204\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 64\n",
      "[64/100] ------> Training :  Accuracy : 100.0\n",
      "[64/100] ------> Training :  Loss     : 0.001152767806913495\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[64/100] ------> Testing  :  Accuracy : 62.0\n",
      "[64/100] ------> Testing  :  Loss     : 2.1662944058563802\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 65\n",
      "[65/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[65/100] ------> Training :  Loss     : 0.003594916681432982\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[65/100] ------> Testing  :  Accuracy : 60.0\n",
      "[65/100] ------> Testing  :  Loss     : 2.210290143839144\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 66\n",
      "[66/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[66/100] ------> Training :  Loss     : 0.011772264738754966\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[66/100] ------> Testing  :  Accuracy : 61.0\n",
      "[66/100] ------> Testing  :  Loss     : 2.171179238864159\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 67\n",
      "[67/100] ------> Training :  Accuracy : 100.0\n",
      "[67/100] ------> Training :  Loss     : 0.0006506588990656093\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[67/100] ------> Testing  :  Accuracy : 60.0\n",
      "[67/100] ------> Testing  :  Loss     : 2.32170960663163\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 68\n",
      "[68/100] ------> Training :  Accuracy : 100.0\n",
      "[68/100] ------> Training :  Loss     : 0.0008687608953790852\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[68/100] ------> Testing  :  Accuracy : 60.0\n",
      "[68/100] ------> Testing  :  Loss     : 2.4308883874664526\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 69\n",
      "[69/100] ------> Training :  Accuracy : 100.0\n",
      "[69/100] ------> Training :  Loss     : 0.00042974691623726667\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[69/100] ------> Testing  :  Accuracy : 60.0\n",
      "[69/100] ------> Testing  :  Loss     : 2.451426759527198\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 70\n",
      "[70/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[70/100] ------> Training :  Loss     : 0.0004906965058902773\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[70/100] ------> Testing  :  Accuracy : 59.0\n",
      "[70/100] ------> Testing  :  Loss     : 2.5376613897986187\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 71\n",
      "[71/100] ------> Training :  Accuracy : 100.0\n",
      "[71/100] ------> Training :  Loss     : 0.0005226553994266433\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[71/100] ------> Testing  :  Accuracy : 61.0\n",
      "[71/100] ------> Testing  :  Loss     : 2.5696079033450734\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 72\n",
      "[72/100] ------> Training :  Accuracy : 100.0\n",
      "[72/100] ------> Training :  Loss     : 0.00021102000650487414\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[72/100] ------> Testing  :  Accuracy : 60.0\n",
      "[72/100] ------> Testing  :  Loss     : 2.638198506242622\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 73\n",
      "[73/100] ------> Training :  Accuracy : 100.0\n",
      "[73/100] ------> Training :  Loss     : 0.00015038497317767423\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[73/100] ------> Testing  :  Accuracy : 59.0\n",
      "[73/100] ------> Testing  :  Loss     : 2.80834952260591\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 74\n",
      "[74/100] ------> Training :  Accuracy : 100.0\n",
      "[74/100] ------> Training :  Loss     : 0.0002515989926404914\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[74/100] ------> Testing  :  Accuracy : 59.0\n",
      "[74/100] ------> Testing  :  Loss     : 2.831045562590981\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 75\n",
      "[75/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[75/100] ------> Training :  Loss     : 0.004620553651802777\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[75/100] ------> Testing  :  Accuracy : 61.0\n",
      "[75/100] ------> Testing  :  Loss     : 2.7037580431415815\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 76\n",
      "[76/100] ------> Training :  Accuracy : 100.0\n",
      "[76/100] ------> Training :  Loss     : 0.0001897040505519354\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[76/100] ------> Testing  :  Accuracy : 62.0\n",
      "[76/100] ------> Testing  :  Loss     : 2.6013377916217464\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 77\n",
      "[77/100] ------> Training :  Accuracy : 100.0\n",
      "[77/100] ------> Training :  Loss     : 0.00042263308335781593\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[77/100] ------> Testing  :  Accuracy : 62.0\n",
      "[77/100] ------> Testing  :  Loss     : 2.441703789641463\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 78\n",
      "[78/100] ------> Training :  Accuracy : 100.0\n",
      "[78/100] ------> Training :  Loss     : 0.00024383386780484393\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[78/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[78/100] ------> Testing  :  Loss     : 2.5948743532486858\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 79\n",
      "[79/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[79/100] ------> Training :  Loss     : 0.11243367468276783\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[79/100] ------> Testing  :  Accuracy : 57.99999999999999\n",
      "[79/100] ------> Testing  :  Loss     : 2.9396983057099426\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 80\n",
      "[80/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[80/100] ------> Training :  Loss     : 0.18391531708646205\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[80/100] ------> Testing  :  Accuracy : 61.0\n",
      "[80/100] ------> Testing  :  Loss     : 2.30536861324852\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 81\n",
      "[81/100] ------> Training :  Accuracy : 98.85714285714286\n",
      "[81/100] ------> Training :  Loss     : 0.006497458568896847\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[81/100] ------> Testing  :  Accuracy : 62.0\n",
      "[81/100] ------> Testing  :  Loss     : 1.8646265610697077\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 82\n",
      "[82/100] ------> Training :  Accuracy : 99.14285714285714\n",
      "[82/100] ------> Training :  Loss     : 0.02256705205316741\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[82/100] ------> Testing  :  Accuracy : 64.0\n",
      "[82/100] ------> Testing  :  Loss     : 1.4609469634396641\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 83\n",
      "[83/100] ------> Training :  Accuracy : 100.0\n",
      "[83/100] ------> Training :  Loss     : 0.008717060280384133\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[83/100] ------> Testing  :  Accuracy : 63.0\n",
      "[83/100] ------> Testing  :  Loss     : 1.6943871163306188\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 84\n",
      "[84/100] ------> Training :  Accuracy : 99.57142857142857\n",
      "[84/100] ------> Training :  Loss     : 0.002599793005690647\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[84/100] ------> Testing  :  Accuracy : 59.0\n",
      "[84/100] ------> Testing  :  Loss     : 1.78238377139997\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 85\n",
      "[85/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[85/100] ------> Training :  Loss     : 0.06127535438402581\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[85/100] ------> Testing  :  Accuracy : 67.0\n",
      "[85/100] ------> Testing  :  Loss     : 1.7219581592396451\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 86\n",
      "[86/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[86/100] ------> Training :  Loss     : 0.0031523010300149494\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[86/100] ------> Testing  :  Accuracy : 63.0\n",
      "[86/100] ------> Testing  :  Loss     : 1.74662052276348\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 87\n",
      "[87/100] ------> Training :  Accuracy : 99.28571428571429\n",
      "[87/100] ------> Training :  Loss     : 0.005415148911496718\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[87/100] ------> Testing  :  Accuracy : 65.0\n",
      "[87/100] ------> Testing  :  Loss     : 1.7575323420140254\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 88\n",
      "[88/100] ------> Training :  Accuracy : 100.0\n",
      "[88/100] ------> Training :  Loss     : 0.02335349151423613\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[88/100] ------> Testing  :  Accuracy : 60.0\n",
      "[88/100] ------> Testing  :  Loss     : 1.715275140381686\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 89\n",
      "[89/100] ------> Training :  Accuracy : 100.0\n",
      "[89/100] ------> Training :  Loss     : 0.001995150516086597\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[89/100] ------> Testing  :  Accuracy : 65.0\n",
      "[89/100] ------> Testing  :  Loss     : 2.000804813833384\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 90\n",
      "[90/100] ------> Training :  Accuracy : 100.0\n",
      "[90/100] ------> Training :  Loss     : 0.0009095990253881477\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[90/100] ------> Testing  :  Accuracy : 64.0\n",
      "[90/100] ------> Testing  :  Loss     : 2.233156119847865\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 91\n",
      "[91/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[91/100] ------> Training :  Loss     : 0.0006539851628396729\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[91/100] ------> Testing  :  Accuracy : 65.0\n",
      "[91/100] ------> Testing  :  Loss     : 2.2159278119547325\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 92\n",
      "[92/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[92/100] ------> Training :  Loss     : 0.0005915318125114693\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[92/100] ------> Testing  :  Accuracy : 65.0\n",
      "[92/100] ------> Testing  :  Loss     : 2.102856317985115\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 93\n",
      "[93/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[93/100] ------> Training :  Loss     : 0.0005296445626726018\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[93/100] ------> Testing  :  Accuracy : 63.0\n",
      "[93/100] ------> Testing  :  Loss     : 2.2488331858406085\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 94\n",
      "[94/100] ------> Training :  Accuracy : 100.0\n",
      "[94/100] ------> Training :  Loss     : 0.0012174019626851535\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[94/100] ------> Testing  :  Accuracy : 62.0\n",
      "[94/100] ------> Testing  :  Loss     : 2.442066169701684\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 95\n",
      "[95/100] ------> Training :  Accuracy : 99.85714285714286\n",
      "[95/100] ------> Training :  Loss     : 0.0010777621807978293\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[95/100] ------> Testing  :  Accuracy : 65.0\n",
      "[95/100] ------> Testing  :  Loss     : 2.2105384967651616\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 96\n",
      "[96/100] ------> Training :  Accuracy : 100.0\n",
      "[96/100] ------> Training :  Loss     : 0.00046345058149121197\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[96/100] ------> Testing  :  Accuracy : 67.0\n",
      "[96/100] ------> Testing  :  Loss     : 2.277106946553646\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 97\n",
      "[97/100] ------> Training :  Accuracy : 100.0\n",
      "[97/100] ------> Training :  Loss     : 0.00026575229212622806\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[97/100] ------> Testing  :  Accuracy : 64.0\n",
      "[97/100] ------> Testing  :  Loss     : 2.4845988578416156\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 98\n",
      "[98/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[98/100] ------> Training :  Loss     : 0.0017261558437597023\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[98/100] ------> Testing  :  Accuracy : 60.0\n",
      "[98/100] ------> Testing  :  Loss     : 2.514801119793125\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 99\n",
      "[99/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[99/100] ------> Training :  Loss     : 0.014227741465494347\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[99/100] ------> Testing  :  Accuracy : 64.0\n",
      "[99/100] ------> Testing  :  Loss     : 2.2000949544864414\n",
      "______________________________________________________________________________________\n",
      "\n",
      "Epoch : 100\n",
      "[100/100] ------> Training :  Accuracy : 99.71428571428571\n",
      "[100/100] ------> Training :  Loss     : 0.0005656170284552545\n",
      "______________________________________________________________________________________\n",
      "\n",
      "[100/100] ------> Testing  :  Accuracy : 64.0\n",
      "[100/100] ------> Testing  :  Loss     : 2.3290583033315113\n",
      "______________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gru = GRU(\n",
    "    input_dim=input_dim,\n",
    "    seq_len = seq_len,\n",
    "    learning_rate = 5e-3,\n",
    "    mom_coeff = 0.9,\n",
    "    batch_size = 32,\n",
    "    hidden_dim = 25,\n",
    "    output_class = len(np.unique(y))\n",
    ")   \n",
    "\n",
    "gru.fit(\n",
    "    X_train,\n",
    "    y_train_hot,\n",
    "    X_val,\n",
    "    y_val_hot,\n",
    "    epochs=100,\n",
    "    optimizer='RMSProb'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9550b2e4702a604127c6792d41a2c1591bb93c1211ce8ff3e331d10f2ba3f332"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
